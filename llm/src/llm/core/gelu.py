import torch
from torch import nn
import math

class GELU(nn.Module):
    """
    GELU (Gaussian Error Linear Unit) — современная сглаженная функция активации для нейросетей.

    Мотивация и назначение:
    -----------------------
    - GELU используется во всех современных трансформерах (BERT, GPT, Llama) вместо ReLU, поскольку лучше передает градиенты и даёт более "мягкое" обучение.
    - Формирует плавный переход между активированным и неактивированным состоянием, что улучшает устойчивость и общую производительность больших моделей.
    - Дает возможность обучению «решать», насколько сильно и в каких диапазонах нужно передавать сигнал (в отличие от жёсткого ReLU).

    Математическая формула:
    -----------------------
        GELU(x) = 0.5 * x * (1 + tanh( sqrt(2/pi) * (x + 0.044715 * x^3) ))
    - Статья (Hendrycks & Gimpel, 2016): https://arxiv.org/abs/1606.08415
    - В PyTorch с версии 1.4+ встроена как torch.nn.functional.gelu и torch.nn.GELU.

    Как это работает:
    -----------------
    - Для каждого входного значения x:
        - x при больших значениях (большие положительные) почти полностью передается дальше.
        - x при малых (или сильно отрицательных) "заглушается" к нулю.
        - На промежуточных значениях — плавный переход.
    - Является аппроксимацией случайного бинома с гауссовским шумом.

    Args:
    -----
    Нет learnable параметров — GELU работает одинаково для всех входов.

    Пример использования:
    ---------------------
        >>> gelu = GELU()
        >>> x = torch.tensor([-2.0, 0.0, 2.0])
        >>> print(gelu(x))  # тензор из плавно переходящих значений

    References:
    -----------
    - Hendrycks & Gimpel: https://arxiv.org/abs/1606.08415
    - BERT, GPT-2 papers (везде используется GELU)
    """

    def __init__(self):
        super().__init__()
        self.sqrt_2_over_pi = torch.sqrt(torch.tensor(2.0) / math.pi)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Прямой проход через GELU-активацию.

        Args:
        -----
        x : torch.Tensor
            Любой входной тензор.

        Returns:
        --------
        torch.Tensor — тензор той же формы, где к каждому элементу применён GELU.

        Пример:
        -------
            >>> gelu = GELU()
            >>> x = torch.linspace(-3, 3, 7)
            >>> y = gelu(x)
        """
        return (
            0.5
            * x
            * (1 + torch.tanh(self.sqrt_2_over_pi * (x + 0.044715 * torch.pow(x, 3))))
        )
