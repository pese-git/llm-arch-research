# LLM Framework - –§—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π

–ú–æ–¥—É–ª—å–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è, –æ–±—É—á–µ–Ω–∏—è –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –±–æ–ª—å—à–∏—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (LLM) —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä (GPT, LLaMA –∏ –¥—Ä.).

## üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞ –ø–æ –º–æ–¥—É–ª—å–Ω–æ–º—É –ø—Ä–∏–Ω—Ü–∏–ø—É —Å —á–µ—Ç–∫–∏–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏:

```
llm/
‚îú‚îÄ‚îÄ core/                 # –ë–∞–∑–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
‚îÇ   ‚îú‚îÄ‚îÄ base_model.py    # –ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –º–æ–¥–µ–ª–µ–π
‚îÇ   ‚îú‚îÄ‚îÄ cached_decoder.py # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –¥–µ–∫–æ–¥–µ—Ä —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
‚îÇ   ‚îú‚îÄ‚îÄ decoder.py       # –ë–∞–∑–æ–≤—ã–π –¥–µ–∫–æ–¥–µ—Ä
‚îÇ   ‚îú‚îÄ‚îÄ multi_head_attention.py # –ú–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ
‚îÇ   ‚îú‚îÄ‚îÄ head_attention.py # –û–¥–Ω–æ-–≥–æ–ª–æ–≤–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ
‚îÇ   ‚îú‚îÄ‚îÄ feed_forward.py  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π FFN —Å–ª–æ–π
‚îÇ   ‚îú‚îÄ‚îÄ token_embeddings.py # –í–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤
‚îÇ   ‚îú‚îÄ‚îÄ positional_embeddings.py # –ê–±—Å–æ–ª—é—Ç–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
‚îÇ   ‚îú‚îÄ‚îÄ rope.py          # Rotary Positional Embeddings (RoPE)
‚îÇ   ‚îú‚îÄ‚îÄ rms_norm.py      # RMS Normalization
‚îÇ   ‚îú‚îÄ‚îÄ swi_glu.py       # SwiGLU –∞–∫—Ç–∏–≤–∞—Ü–∏—è
‚îÇ   ‚îú‚îÄ‚îÄ silu.py          # SiLU –∞–∫—Ç–∏–≤–∞—Ü–∏—è
‚îÇ   ‚îî‚îÄ‚îÄ gelu.py          # GELU –∞–∫—Ç–∏–≤–∞—Ü–∏—è
‚îú‚îÄ‚îÄ models/              # –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π
‚îÇ   ‚îú‚îÄ‚îÄ gpt/            # GPT –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gpt.py      # –ë–∞–∑–æ–≤–∞—è GPT
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gpt2.py     # GPT-2 —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ llama/          # LLaMA –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llama.py    # LLaMA —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ mistral/        # Mistral –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
‚îÇ       ‚îú‚îÄ‚îÄ mistral.py  # Mistral —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
‚îÇ       ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ tokenizers/          # –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã
‚îÇ   ‚îú‚îÄ‚îÄ base_tokenizer.py # –ë–∞–∑–æ–≤—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
‚îÇ   ‚îî‚îÄ‚îÄ bpe_tokenizer.py # BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
‚îú‚îÄ‚îÄ datasets/            # –†–∞–±–æ—Ç–∞ —Å –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏
‚îÇ   ‚îú‚îÄ‚îÄ text_dataset.py    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
‚îÇ   ‚îî‚îÄ‚îÄ streaming_text_dataset.py # –°—Ç—Ä–∏–º–∏–Ω–≥–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
‚îî‚îÄ‚îÄ training/           # –£—Ç–∏–ª–∏—Ç—ã –æ–±—É—á–µ–Ω–∏—è
    ‚îú‚îÄ‚îÄ trainer.py      # –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π —Ü–∏–∫–ª
    ‚îú‚îÄ‚îÄ optimizer.py    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã
    ‚îî‚îÄ‚îÄ scheduler.py    # –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
```

## üß© –ö–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã

### BaseModel (`core/base_model.py`)
**–ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å** –¥–ª—è –≤—Å–µ—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –µ–¥–∏–Ω—ã–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º.

```python
class BaseModel(nn.Module, ABC):
    @abstractmethod
    def forward(self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ –º–æ–¥–µ–ª–∏."""
    
    @abstractmethod
    def generate(self, input_ids: torch.Tensor, max_length: int = 50) -> torch.Tensor:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞."""
```

### CachedDecoder (`core/cached_decoder.py`)
**–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –¥–µ–∫–æ–¥–µ—Ä** —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π dependency injection –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è KV-–ø–∞–º—è—Ç–∏.

```python
CachedDecoder(
    feed_forward_layer=FeedForward(...),  # –∏–ª–∏ SwiGLU
    norm_layer=nn.LayerNorm,              # –∏–ª–∏ RMSNorm
    rope=RoPE(...),                       # –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
    # ... –¥—Ä—É–≥–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
)
```

### RoPE (`core/rope.py`)
**Rotary Positional Embeddings** - —Ä–æ—Ç–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏.

**–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Å–Ω–æ–≤–∞:**
```
Œ∏_i = base^(-2i/d)
q'_m = q_m * cos(mŒ∏_i) + rotate(q_m) * sin(mŒ∏_i)
```

### RMSNorm (`core/rms_norm.py`)
**Root Mean Square Normalization** - —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –±–µ–∑ —Å—Ä–µ–¥–Ω–µ–≥–æ.

**–§–æ—Ä–º—É–ª–∞:**
```
RMSNorm(x) = (x / RMS(x)) * w
–≥–¥–µ RMS(x) = sqrt(mean(x¬≤) + eps)
```

### SwiGLU (`core/swi_glu.py`)
**Swish-Gated Linear Unit** - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è —Å gating mechanism.

**–§–æ—Ä–º—É–ª–∞:**
```
SwiGLU(x) = Swish(xW_g + b_g) ‚äô (xW_u + b_u) * W_d + b_d
```

## üöÄ –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–π GPT –º–æ–¥–µ–ª–∏
```python
from llm.models.gpt import GPT

config = {
    "vocab_size": 50257,
    "embed_dim": 768,
    "num_heads": 12,
    "num_layers": 12,
    "max_position_embeddings": 1024,
    "dropout": 0.1
}

model = GPT(config)
```

### –°–æ–∑–¥–∞–Ω–∏–µ GPT2 –º–æ–¥–µ–ª–∏
```python
from llm.models.gpt import GPT2

config = {
    "vocab_size": 50257,
    "embed_dim": 768,
    "num_heads": 12,
    "num_layers": 12,
    "max_position_embeddings": 1024,
    "dropout": 0.1
}

model = GPT2(config)
```

### –°–æ–∑–¥–∞–Ω–∏–µ LLaMA –º–æ–¥–µ–ª–∏
```python
from llm.models.llama import Llama
from llm.core.swi_glu import SwiGLU
from llm.core.rms_norm import RMSNorm

config = {
    "vocab_size": 32000,
    "embed_dim": 4096,
    "num_heads": 32,
    "num_layers": 32,
    "max_position_embeddings": 2048,
    "dropout": 0.1
}

model = Llama(config)
```

### –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
```python
# –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
output = model(input_ids, attention_mask)

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
generated = model.generate(input_ids, max_length=100)
```

## üìä –í—Ö–æ–¥–Ω—ã–µ –∏ –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ

### –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:
- `input_ids`: `Tensor[int64]` —Ñ–æ—Ä–º—ã `[batch_size, seq_len]` - –∏–Ω–¥–µ–∫—Å—ã —Ç–æ–∫–µ–Ω–æ–≤
- `attention_mask`: `Tensor[bool]` —Ñ–æ—Ä–º—ã `[batch_size, seq_len]` - –º–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è
- `cache`: `List[Tuple[Tensor, Tensor]]` - –∫—ç—à –∫–ª—é—á–µ–π-–∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

### –í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:
- `logits`: `Tensor[float32]` —Ñ–æ—Ä–º—ã `[batch_size, seq_len, vocab_size]` - –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤
- `cache`: `List[Tuple[Tensor, Tensor]]` - –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –∫—ç—à (–ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏)

## üèÜ –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

### GPT (Original) –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏
- ‚úÖ –ú–Ω–æ–≥–æ–≥–æ–ª–æ–≤–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ
- ‚úÖ Layer Normalization (–ø–æ—Å–ª–µ –≤–Ω–∏–º–∞–Ω–∏—è –∏ FFN)
- ‚úÖ GELU –∞–∫—Ç–∏–≤–∞—Ü–∏—è
- ‚úÖ Learned positional embeddings
- ‚úÖ –ë–∞–∑–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä-–¥–µ–∫–æ–¥–µ—Ä–∞

### GPT-2 –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏
- ‚úÖ Layer Normalization (–ø–µ—Ä–µ–¥ –≤–Ω–∏–º–∞–Ω–∏–µ–º –∏ FFN)
- ‚úÖ GELU –∞–∫—Ç–∏–≤–∞—Ü–∏—è
- ‚úÖ Learned positional embeddings
- ‚úÖ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ KV –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
- ‚úÖ –£–ª—É—á—à–µ–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ—ë–≤

### LLaMA –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏
- ‚úÖ Rotary Positional Embeddings (RoPE)
- ‚úÖ RMS Normalization –≤–º–µ—Å—Ç–æ LayerNorm
- ‚úÖ SwiGLU –∞–∫—Ç–∏–≤–∞—Ü–∏—è –≤–º–µ—Å—Ç–æ GELU
- ‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–µ–∫–æ–¥–µ—Ä–∞
- ‚úÖ –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ KV-–ø–∞–º—è—Ç–∏

### Mistral –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏
- ‚úÖ Sliding Window Attention (–æ–∫–æ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ)
- ‚úÖ Grouped Query Attention (GQA)
- ‚úÖ RoPE
- ‚úÖ RMSNorm
- ‚úÖ –†–∞–∑–¥–µ–ª—ë–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–∞ –±–ª–æ–∫–∏ —Å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –ø–∞–º—è—Ç—å—é
- ‚úÖ –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å HuggingFace —á–µ—Ä–µ–∑ hf-proxy

## ü§ù –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å HuggingFace –∏ BPE

- –í—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ —á–µ—Ä–µ–∑ HuggingFace (—Å–º. hf-proxy).
- hf-proxy ‚Äî —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –º–æ–¥—É–ª—å! –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å –±—É–¥—É—â–∏–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ Transformers –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç—Å—è; API –º–æ–∂–µ—Ç –º–µ–Ω—è—Ç—å—Å—è.
- –î–æ–ø—É—Å–∫–∞–µ—Ç—Å—è –∑–∞–≥—Ä—É–∑–∫–∞/–∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π –≤ —Ñ–æ—Ä–º–∞—Ç HF –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã Transformers.
- –î–ª—è –∑–∞–ø—É—Å–∫–∞ –º–æ–¥–µ–ª–µ–π —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞–º–∏ HF –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ `hf-proxy` –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –∏–∑ `experiments/hf_integration/`.

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

–ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤:
```bash
cd llm
python -m pytest tests/ -v
```

**–°—Ç–∞—Ç—É—Å —Ç–µ—Å—Ç–æ–≤:** ‚úÖ 101+ —Ç–µ—Å—Ç, –æ—Ö–≤–∞—á–µ–Ω—ã –≤—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã (—è–¥—Ä–æ, —è–¥—Ä–æ-—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã, –æ–±—É—á–µ–Ω–∏–µ)

## üìö –ù–∞—É—á–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏

### –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
–û—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ –º–µ—Ö–∞–Ω–∏–∑–º–µ **–≤–Ω–∏–º–∞–Ω–∏—è**, –ø–æ–∑–≤–æ–ª—è—é—â–µ–º –º–æ–¥–µ–ª–∏ –≤–∑–≤–µ—à–∏–≤–∞—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç–µ–π –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.

**–§–æ—Ä–º—É–ª–∞ –≤–Ω–∏–º–∞–Ω–∏—è:**
```
Attention(Q, K, V) = softmax(Q¬∑K·µÄ/‚àöd_k)¬∑V
```

### RoPE (Rotary Positional Embeddings)
–ò–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ **–≤—Ä–∞—â–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤** –≤ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ.

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
- –õ—É—á—à–∞—è —ç–∫—Å—Ç—Ä–∞–ø–æ–ª—è—Ü–∏—è –Ω–∞ –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–æ—Ä–º—ã –≤–µ–∫—Ç–æ—Ä–æ–≤

### RMSNorm vs LayerNorm
**RMSNorm** —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –≤—ã—á–∏—Ç–∞–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–≥–æ –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–º –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π.

### SwiGLU vs GELU
**SwiGLU** —Å gating mechanism –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –±–ª–∞–≥–æ–¥–∞—Ä—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤—ã–±–æ—Ä–æ—á–Ω–æ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.

## üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ

–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞ —Å —É—á–µ—Ç–æ–º **—Ä–∞—Å—à–∏—Ä—è–µ–º–æ—Å—Ç–∏**. –î–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã:

1. **–ù–∞—Å–ª–µ–¥–æ–≤–∞—Ç—å—Å—è** –æ—Ç `BaseModel`
2. **–†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å** –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã `forward()` –∏ `generate()`
3. **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å** –º–æ–¥—É–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏–∑ `core/`
4. **–î–æ–±–∞–≤–∏—Ç—å** –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏

### –ü—Ä–∏–º–µ—Ä —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è:
```python
class NewModel(BaseModel):
    def __init__(self, config):
        super().__init__(config)
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≥–æ—Ç–æ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.decoder = CachedDecoder(...)
        
    def forward(self, input_ids, attention_mask=None):
        # –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä—è–º–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞
        pass
```

## üìÑ –õ–∏—Ü–µ–Ω–∑–∏—è

–ü—Ä–æ–µ–∫—Ç —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ—Ç—Å—è –ø–æ–¥ MIT License.
