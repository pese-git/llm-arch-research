"""
–û–±—â–∏–µ —É—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏ –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞—Ö.
"""

import os
from typing import List, Tuple
from .configs import TRAIN_TEXTS, PATHS


def load_training_data(split_ratio: float = 0.8) -> Tuple[List[str], List[str]]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ä–∞–∑–¥–µ–ª—è–µ—Ç –Ω–∞ train/validation.
    
    Args:
        split_ratio: –î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        
    Returns:
        Tuple: (train_texts, val_texts)
    """
    train_size = int(len(TRAIN_TEXTS) * split_ratio)
    train_data = TRAIN_TEXTS[:train_size]
    val_data = TRAIN_TEXTS[train_size:]
    
    return train_data, val_data


def ensure_directories():
    """–°–æ–∑–¥–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç."""
    directories = [
        "checkpoints",
        "checkpoints/gpt-bpe", 
        "checkpoints/hf-bpe-tokenizer",
        "checkpoints/hf-trained",
        "checkpoints/hf-trained-proxy",
        "logs"
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)


def get_model_paths(experiment_type: str = "llm_only") -> dict:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–∏–ø–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞.
    
    Args:
        experiment_type: –¢–∏–ø —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ ('llm_only' –∏–ª–∏ 'hf_integration')
        
    Returns:
        dict: –°–ª–æ–≤–∞—Ä—å —Å –ø—É—Ç—è–º–∏
    """
    base_paths = PATHS.copy()
    
    if experiment_type == "hf_integration":
        base_paths.update({
            "model": base_paths["hf_model"],
            "tokenizer": base_paths["hf_tokenizer"]
        })
    else:  # llm_only
        base_paths.update({
            "model": base_paths["gpt_bpe_model"],
            "tokenizer": base_paths["bpe_tokenizer"]
        })
    
    return base_paths


def print_experiment_info(experiment_name: str, config: dict):
    """
    –í—ã–≤–æ–¥–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–ø—É—Å–∫–∞–µ–º–æ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–µ.
    
    Args:
        experiment_name: –ù–∞–∑–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    """
    print("=" * 70)
    print(f"üöÄ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: {experiment_name}")
    print("=" * 70)
    print("üìä –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:")
    for key, value in config.items():
        print(f"   {key}: {value}")
    print()


def save_experiment_results(results: dict, filepath: str):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –≤ —Ñ–∞–π–ª.
    
    Args:
        results: –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        filepath: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    """
    import json
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {filepath}")


def load_experiment_results(filepath: str) -> dict:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞.
    
    Args:
        filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        
    Returns:
        dict: –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    """
    import json
    
    if not os.path.exists(filepath):
        return {}
    
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)


class ExperimentLogger:
    """
    –õ–æ–≥–≥–µ—Ä –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤.
    """
    
    def __init__(self, experiment_name: str):
        self.experiment_name = experiment_name
        self.metrics = {}
    
    def log_metric(self, name: str, value: float):
        """–õ–æ–≥–∏—Ä—É–µ—Ç –º–µ—Ç—Ä–∏–∫—É."""
        if name not in self.metrics:
            self.metrics[name] = []
        self.metrics[name].append(value)
        print(f"üìà {name}: {value:.4f}")
    
    def log_step(self, step: int, loss: float, **kwargs):
        """–õ–æ–≥–∏—Ä—É–µ—Ç —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è."""
        print(f"üìä Step {step}: loss={loss:.4f}", end="")
        for key, value in kwargs.items():
            print(f", {key}={value:.4f}", end="")
        print()
    
    def log_epoch(self, epoch: int, train_loss: float, val_loss: float = None):
        """–õ–æ–≥–∏—Ä—É–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —ç–ø–æ—Ö–∏."""
        print(f"üéØ Epoch {epoch}: train_loss={train_loss:.4f}", end="")
        if val_loss is not None:
            print(f", val_loss={val_loss:.4f}", end="")
        print()
    
    def save_logs(self, filepath: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª–æ–≥–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞."""
        import json
        
        logs = {
            "experiment_name": self.experiment_name,
            "metrics": self.metrics
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(logs, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ –õ–æ–≥–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {filepath}")
