"""
Общие конфигурации для экспериментов.
"""

# === Данные для обучения ===
TRAIN_TEXTS = [
    "Мир программирования прекрасен и удивителен.",
    "GPT модели учатся предсказывать следующий токен в последовательности.",
    "Трансформеры революционно изменили обработку естественного языка.",
    "Обучение больших языковых моделей требует значительных вычислительных ресурсов и больших объемов данных.",
    "Искусственный интеллект продолжает развиваться стремительными темпами.",
    "Глубокое обучение позволяет решать сложные задачи компьютерного зрения.",
    "Нейронные сети имитируют работу человеческого мозга.",
    "Машинное обучение находит применение в различных областях науки и техники.",
    "Python является одним из самых популярных языков программирования для анализа данных.",
    "Обработка естественного языка позволяет компьютерам понимать человеческую речь.",
    "Рекуррентные нейронные сети хорошо подходят для работы с последовательностями.",
    "Сверточные нейронные сети эффективны для обработки изображений.",
    "Обучение с подкреплением используется для создания игровых ИИ.",
    "Генеративные состязательные сети могут создавать реалистичные изображения.",
    "Автоэнкодеры используются для сжатия данных и обучения представлений.",
]

# === Конфигурации моделей ===

# Базовая конфигурация GPT
BASE_GPT_CONFIG = {
    "vocab_size": None,  # Будет установлен динамически
    "embed_dim": 256,
    "num_heads": 4,
    "num_layers": 4,
    "max_position_embeddings": 128,
    "dropout": 0.1,
}

# Конфигурация для маленькой модели (быстрое тестирование)
SMALL_GPT_CONFIG = {
    "vocab_size": None,
    "embed_dim": 128,
    "num_heads": 2,
    "num_layers": 2,
    "max_position_embeddings": 64,
    "dropout": 0.1,
}

# Конфигурация для большой модели (качественное обучение)
LARGE_GPT_CONFIG = {
    "vocab_size": None,
    "embed_dim": 512,
    "num_heads": 8,
    "num_layers": 6,
    "max_position_embeddings": 256,
    "dropout": 0.1,
}

# === Конфигурации токенизатора ===
BPE_CONFIG = {
    "vocab_size": 1000,
    "special_tokens": ["<pad>", "<unk>", "<bos>", "<eos>"],
}

# === Конфигурации обучения ===
TRAINING_CONFIG = {
    "learning_rate": 3e-4,
    "batch_size": 2,
    "num_epochs": 3,
    "warmup_steps": 50,
    "gradient_clip": 1.0,
}

# === Конфигурации генерации ===
GENERATION_CONFIG = {
    "max_new_tokens": 50,
    "temperature": 0.7,
    "do_sample": True,
    "top_k": None,
    "top_p": None,
}

# === Пути для сохранения ===
PATHS = {
    "bpe_tokenizer": "checkpoints/bpe_tokenizer.json",
    "gpt_bpe_model": "checkpoints/gpt-bpe/model.pt",
    "gpt_bpe_config": "checkpoints/gpt-bpe/config.json",
    "hf_tokenizer": "checkpoints/hf-bpe-tokenizer",
    "hf_model": "checkpoints/hf-trained",
    "hf_proxy_model": "checkpoints/hf-trained-proxy",
}

# === Тестовые промпты ===
TEST_PROMPTS = [
    "Искусственный",
    "Нейронные",
    "Машинное",
    "Глубокое",
    "Python",
    "Трансформеры",
    "Обучение",
    "Программирование",
]
