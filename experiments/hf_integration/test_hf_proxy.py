#!/usr/bin/env python3
"""
Test: test_hf_proxy.py
Description: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ hf-proxy –±–µ–∑ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π.
"""

import torch
import os
import sys

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ shared –º–æ–¥—É–ª—è–º
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from llm.models.gpt import GPT
from llm.tokenizers import BPETokenizer
from hf_proxy import HFAdapter, HFTokenizerAdapter

from shared.configs import (
    TRAIN_TEXTS,
    BASE_GPT_CONFIG,
    BPE_CONFIG,
    TEST_PROMPTS,
    GENERATION_CONFIG,
)


def test_basic_hf_integration():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –±–∞–∑–æ–≤—É—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é hf-proxy."""
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ hf-proxy...")

    # === –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ ===
    print("1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    llm_tokenizer = BPETokenizer()
    llm_tokenizer.train(
        texts=TRAIN_TEXTS,
        vocab_size=BPE_CONFIG["vocab_size"],
        special_tokens=BPE_CONFIG["special_tokens"],
    )

    hf_tokenizer = HFTokenizerAdapter(llm_tokenizer)
    print(f"   ‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ–∑–¥–∞–Ω (vocab_size={hf_tokenizer.vocab_size})")

    # === –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ ===
    print("2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏...")
    model_config = BASE_GPT_CONFIG.copy()
    model_config["vocab_size"] = hf_tokenizer.vocab_size

    llm_model = GPT(model_config)
    hf_model = HFAdapter.from_llm_model(llm_model)
    print(f"   ‚úÖ –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞")

    # === –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ ===
    print("3. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏...")
    test_texts = ["–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏"]

    for text in test_texts:
        print(f"   üìù –¢–µ–∫—Å—Ç: '{text}'")

        # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        original_tokens = llm_tokenizer.encode(text)
        print(f"      –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π: {len(original_tokens)} —Ç–æ–∫–µ–Ω–æ–≤")

        # HF –∞–¥–∞–ø—Ç–µ—Ä
        hf_inputs = hf_tokenizer(text, return_tensors="pt")
        print(f"      HF –∞–¥–∞–ø—Ç–µ—Ä: {hf_inputs['input_ids'].shape}")

        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        decoded = hf_tokenizer.decode(hf_inputs["input_ids"][0])
        print(f"      –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π: '{decoded}'")

    # === –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ forward pass ===
    print("4. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ forward pass...")
    for text in test_texts:
        hf_inputs = hf_tokenizer(text, return_tensors="pt")

        with torch.no_grad():
            outputs = hf_model(**hf_inputs)

        print(f"   üìù '{text}' -> logits: {outputs.logits.shape}")

    # === –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ===
    print("5. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏...")
    hf_model.eval()

    for prompt in TEST_PROMPTS[:3]:
        print(f"   üî§ –ü—Ä–æ–º–ø—Ç: '{prompt}'")

        try:
            inputs = hf_tokenizer(prompt, return_tensors="pt")

            with torch.no_grad():
                generated = hf_model.generate(
                    input_ids=inputs["input_ids"],
                    max_new_tokens=10,
                    do_sample=True,
                    temperature=0.8,
                )

            generated_text = hf_tokenizer.decode(generated[0], skip_special_tokens=True)
            print(f"      üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç: '{generated_text}'")

        except Exception as e:
            print(f"      ‚ùå –û—à–∏–±–∫–∞: {e}")

    # === –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è/–∑–∞–≥—Ä—É–∑–∫–∏ ===
    print("6. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è/–∑–∞–≥—Ä—É–∑–∫–∏...")
    try:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        hf_tokenizer.save_pretrained("test_save/tokenizer")
        print("   ‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        HFAdapter.save_pretrained(hf_model, "test_save/model", tokenizer=hf_tokenizer)
        print("   ‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        loaded_tokenizer = HFTokenizerAdapter.from_pretrained("test_save/tokenizer")
        print(f"   ‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω (vocab_size={loaded_tokenizer.vocab_size})")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        model_path = os.path.join("test_save/model", "pytorch_model.bin")
        loaded_model = HFAdapter.from_pretrained(model_path)
        print("   ‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        test_input = hf_tokenizer("–¢–µ—Å—Ç", return_tensors="pt")
        with torch.no_grad():
            loaded_outputs = loaded_model(**test_input)
        print(
            f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç (logits: {loaded_outputs.logits.shape})"
        )

    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è/–∑–∞–≥—Ä—É–∑–∫–∏: {e}")

    print("\nüéâ –ë–∞–∑–æ–≤–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ hf-proxy –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")


def test_hf_tokenizer_methods():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã HF —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞."""
    print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ HF —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")

    # –°–æ–∑–¥–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    llm_tokenizer = BPETokenizer()
    llm_tokenizer.train(
        texts=TRAIN_TEXTS[:5],
        vocab_size=500,
        special_tokens=BPE_CONFIG["special_tokens"],
    )

    hf_tokenizer = HFTokenizerAdapter(llm_tokenizer)

    test_text = "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –∏ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ –º–µ—Ç–æ–¥—ã
    print("1. –ú–µ—Ç–æ–¥ __call__:")
    result = hf_tokenizer(test_text, return_tensors="pt")
    print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")

    print("2. –ú–µ—Ç–æ–¥ encode:")
    encoded = hf_tokenizer.encode(test_text)
    print(f"   –ó–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ: {encoded}")

    print("3. –ú–µ—Ç–æ–¥ decode:")
    decoded = hf_tokenizer.decode(encoded)
    print(f"   –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ: '{decoded}'")

    print("4. –ú–µ—Ç–æ–¥ tokenize:")
    tokens = hf_tokenizer.tokenize(test_text)
    print(f"   –¢–æ–∫–µ–Ω—ã: {tokens}")

    print("5. –ú–µ—Ç–æ–¥ get_vocab:")
    vocab = hf_tokenizer.get_vocab()
    print(f"   –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(vocab)}")

    print("‚úÖ –í—Å–µ –º–µ—Ç–æ–¥—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —Ä–∞–±–æ—Ç–∞—é—Ç!")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""
    print("=" * 60)
    print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï HF-PROXY")
    print("=" * 60)

    try:
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –±–∞–∑–æ–≤—É—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é
        test_basic_hf_integration()

        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–µ—Ç–æ–¥—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        test_hf_tokenizer_methods()

        print("\n" + "=" * 60)
        print("üéâ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´ –£–°–ü–ï–®–ù–û!")
        print("=" * 60)
        print("\nüìö –ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:")
        print("   ‚úÖ –°–æ–∑–¥–∞–Ω–∏–µ HF –∞–¥–∞–ø—Ç–µ—Ä–∞ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞")
        print("   ‚úÖ –°–æ–∑–¥–∞–Ω–∏–µ HF –∞–¥–∞–ø—Ç–µ—Ä–∞ –¥–ª—è –º–æ–¥–µ–ª–∏")
        print("   ‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ")
        print("   ‚úÖ Forward pass —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å")
        print("   ‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞")
        print("   ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π")
        print("   ‚úÖ –í—Å–µ –º–µ—Ç–æ–¥—ã HF —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞")

    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
