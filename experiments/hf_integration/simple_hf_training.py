#!/usr/bin/env python3
"""
Experiment: simple_hf_training.py
Description: –£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ GPT –º–æ–¥–µ–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º hf-proxy.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä—É—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ —Å–ª–æ–∂–Ω–æ–≥–æ HuggingFace Trainer.
"""

import torch
import torch.nn as nn
import os
import sys
import json

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ shared –º–æ–¥—É–ª—è–º
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from llm.models.gpt import GPT
from llm.tokenizers import BPETokenizer
from hf_proxy import HFAdapter, HFTokenizerAdapter

from shared.configs import (
    TRAIN_TEXTS,
    BASE_GPT_CONFIG,
    BPE_CONFIG,
    TRAINING_CONFIG,
    PATHS,
    TEST_PROMPTS,
)


def create_dataset(hf_tokenizer, texts, max_length=128):
    """
    –°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ—Å—Ç–æ–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.

    Args:
        hf_tokenizer: –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
        max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

    Returns:
        list: –°–ø–∏—Å–æ–∫ —Ç–µ–Ω–∑–æ—Ä–æ–≤ input_ids
    """
    dataset = []

    for text in texts:
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
        inputs = hf_tokenizer(
            text,
            max_length=max_length,
            truncation=True,
            padding=False,
            return_tensors="pt",
        )

        input_ids = inputs["input_ids"][0]

        # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫–∏ –¥–ª—è —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è
        labels = input_ids.clone()

        dataset.append({"input_ids": input_ids, "labels": labels})

    return dataset


def manual_training_loop(hf_model, hf_tokenizer, train_texts, val_texts, config):
    """
    –†—É—á–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Trainer.

    Args:
        hf_model: –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        hf_tokenizer: –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        train_texts: –¢–µ–∫—Å—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        val_texts: –¢–µ–∫—Å—Ç—ã –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è

    Returns:
        dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è
    """
    print("üéØ –ó–∞–ø—É—Å–∫ —Ä—É—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è...")

    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã
    train_dataset = create_dataset(hf_tokenizer, train_texts)
    val_dataset = create_dataset(hf_tokenizer, val_texts)

    print(f"üìä –î–∞–Ω–Ω—ã–µ: {len(train_dataset)} train, {len(val_dataset)} validation")

    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
    optimizer = torch.optim.AdamW(hf_model.parameters(), lr=config["learning_rate"])

    # –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
    loss_fn = nn.CrossEntropyLoss()

    # –û–±—É—á–µ–Ω–∏–µ
    hf_model.train()
    train_losses = []
    val_losses = []

    for epoch in range(config["num_epochs"]):
        print(f"\nüìÖ –≠–ø–æ—Ö–∞ {epoch + 1}/{config['num_epochs']}")

        # –û–±—É—á–µ–Ω–∏–µ
        epoch_train_loss = 0
        for i, batch in enumerate(train_dataset):
            optimizer.zero_grad()

            input_ids = batch["input_ids"].unsqueeze(0)  # [1, seq_len]
            labels = batch["labels"].unsqueeze(0)  # [1, seq_len]

            # Forward pass
            outputs = hf_model(input_ids=input_ids, labels=labels)
            loss = outputs.loss

            # Backward pass
            loss.backward()
            optimizer.step()

            epoch_train_loss += loss.item()

            if i % 5 == 0:
                print(f"   Batch {i}/{len(train_dataset)}: loss = {loss.item():.4f}")

        avg_train_loss = epoch_train_loss / len(train_dataset)
        train_losses.append(avg_train_loss)
        print(f"   üìä –°—Ä–µ–¥–Ω—è—è train loss: {avg_train_loss:.4f}")

        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        hf_model.eval()
        epoch_val_loss = 0
        with torch.no_grad():
            for batch in val_dataset:
                input_ids = batch["input_ids"].unsqueeze(0)
                labels = batch["labels"].unsqueeze(0)

                outputs = hf_model(input_ids=input_ids, labels=labels)
                epoch_val_loss += outputs.loss.item()

        avg_val_loss = epoch_val_loss / len(val_dataset)
        val_losses.append(avg_val_loss)
        print(f"   üìä –°—Ä–µ–¥–Ω—è—è val loss: {avg_val_loss:.4f}")

        hf_model.train()

    return {
        "train_losses": train_losses,
        "val_losses": val_losses,
        "final_train_loss": train_losses[-1],
        "final_val_loss": val_losses[-1],
    }


def test_generation_after_training(hf_model, hf_tokenizer, test_prompts):
    """
    –¢–µ—Å—Ç–∏—Ä—É–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è.

    Args:
        hf_model: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        hf_tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        test_prompts: –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–æ–º–ø—Ç—ã
    """
    print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è...")
    hf_model.eval()

    for prompt in test_prompts[:3]:
        print(f"\nüî§ –ü—Ä–æ–º–ø—Ç: '{prompt}'")

        try:
            inputs = hf_tokenizer(prompt, return_tensors="pt")

            with torch.no_grad():
                generated = hf_model.generate(
                    input_ids=inputs["input_ids"],
                    max_new_tokens=20,
                    do_sample=True,
                    temperature=0.8,
                )

            generated_text = hf_tokenizer.decode(generated[0], skip_special_tokens=True)
            print(f"üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç: '{generated_text}'")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞."""
    print("=" * 60)
    print("üöÄ –£–ü–†–û–©–ï–ù–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï GPT –° HF-PROXY")
    print("=" * 60)

    try:
        # === –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===
        print("üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        train_texts = TRAIN_TEXTS[
            :10
        ]  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        val_texts = TRAIN_TEXTS[10:12]

        print(f"üìä –î–∞–Ω–Ω—ã–µ: {len(train_texts)} train, {len(val_texts)} validation")

        # === –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ ===
        print("üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
        llm_tokenizer = BPETokenizer()
        llm_tokenizer.train(
            texts=train_texts,
            vocab_size=BPE_CONFIG["vocab_size"],
            special_tokens=BPE_CONFIG["special_tokens"],
        )

        hf_tokenizer = HFTokenizerAdapter(llm_tokenizer)
        print(f"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ–∑–¥–∞–Ω (vocab_size={hf_tokenizer.vocab_size})")

        # === –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ ===
        print("üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏...")
        model_config = BASE_GPT_CONFIG.copy()
        model_config["vocab_size"] = hf_tokenizer.vocab_size

        llm_model = GPT(model_config)
        hf_model = HFAdapter.from_llm_model(llm_model)
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞")

        # === –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ –æ–±—É—á–µ–Ω–∏—è ===
        print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ –æ–±—É—á–µ–Ω–∏—è...")
        test_generation_after_training(hf_model, hf_tokenizer, TEST_PROMPTS)

        # === –û–±—É—á–µ–Ω–∏–µ ===
        print(f"\nüéØ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        training_config = {
            "learning_rate": TRAINING_CONFIG["learning_rate"],
            "num_epochs": 2,  # –ú–µ–Ω—å—à–µ —ç–ø–æ—Ö –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            "batch_size": TRAINING_CONFIG["batch_size"],
        }

        results = manual_training_loop(
            hf_model, hf_tokenizer, train_texts, val_texts, training_config
        )

        print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è:")
        print(f"   Final train loss: {results['final_train_loss']:.4f}")
        print(f"   Final val loss: {results['final_val_loss']:.4f}")

        # === –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è ===
        print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è...")
        test_generation_after_training(hf_model, hf_tokenizer, TEST_PROMPTS)

        # === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ ===
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        os.makedirs("checkpoints/hf_simple_trained", exist_ok=True)
        os.makedirs("checkpoints/hf_simple_tokenizer", exist_ok=True)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        hf_tokenizer.save_pretrained("checkpoints/hf_simple_tokenizer")
        print("‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        HFAdapter.save_pretrained(
            hf_model, "checkpoints/hf_simple_trained", tokenizer=hf_tokenizer
        )
        print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results_path = "checkpoints/simple_training_results.json"
        with open(results_path, "w", encoding="utf-8") as f:
            json.dump(
                {
                    "training_config": training_config,
                    "model_config": model_config,
                    "results": results,
                },
                f,
                indent=2,
                ensure_ascii=False,
            )
        print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {results_path}")

        print(f"\nüéâ –£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
        print(f"\nüí° –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏:")
        print(f"   uv run python experiments/hf_integration/generate_with_hf_tools.py")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–µ: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
