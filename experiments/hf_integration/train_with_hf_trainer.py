#!/usr/bin/env python3
"""
Experiment: train_with_hf_trainer.py
Description: –û–±—É—á–µ–Ω–∏–µ GPT –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ HuggingFace Trainer —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º hf-proxy.
–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –∫–∞—Å—Ç–æ–º–Ω—É—é –º–æ–¥–µ–ª—å llm —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ HuggingFace.
"""

import torch
import os
import sys

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ shared –º–æ–¥—É–ª—è–º
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from llm.models.gpt import GPT
from llm.tokenizers import BPETokenizer
from hf_proxy import HFAdapter, HFTokenizerAdapter

from shared.configs import (
    TRAIN_TEXTS, BASE_GPT_CONFIG, BPE_CONFIG, 
    TRAINING_CONFIG, PATHS, TEST_PROMPTS
)
from shared.data import (
    load_training_data, ensure_directories, 
    print_experiment_info, ExperimentLogger
)


def setup_hf_training():
    """
    –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –æ–∫—Ä—É–∂–µ–Ω–∏–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ HuggingFace Trainer.
    
    Returns:
        tuple: (hf_model, hf_tokenizer, llm_tokenizer, model_config)
    """
    print("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ HuggingFace –æ–±—É—á–µ–Ω–∏—è...")
    
    # === –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===
    train_texts, val_texts = load_training_data()
    print(f"üìä –î–∞–Ω–Ω—ã–µ: {len(train_texts)} train, {len(val_texts)} validation")
    
    # === –û–±—É—á–µ–Ω–∏–µ/–∑–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ ===
    if os.path.exists(PATHS["bpe_tokenizer"]):
        print("üìù –ó–∞–≥—Ä—É–∑–∫–∞ BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
        llm_tokenizer = BPETokenizer.load(PATHS["bpe_tokenizer"])
        print(f"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω (vocab_size={llm_tokenizer.get_vocab_size()})")
    else:
        print("üìù –û–±—É—á–µ–Ω–∏–µ BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
        llm_tokenizer = BPETokenizer()
        llm_tokenizer.train(
            texts=TRAIN_TEXTS,
            vocab_size=BPE_CONFIG["vocab_size"],
            special_tokens=BPE_CONFIG["special_tokens"]
        )
        llm_tokenizer.save(PATHS["bpe_tokenizer"])
        print(f"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –æ–±—É—á–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
    
    # === –°–æ–∑–¥–∞–Ω–∏–µ –∞–¥–∞–ø—Ç–µ—Ä–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ ===
    print("üîß –°–æ–∑–¥–∞–Ω–∏–µ –∞–¥–∞–ø—Ç–µ—Ä–∞ HuggingFace –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    hf_tokenizer = HFTokenizerAdapter(llm_tokenizer)
    print(f"‚úÖ –ê–¥–∞–ø—Ç–µ—Ä —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ —Å–æ–∑–¥–∞–Ω")
    
    # === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ ===
    model_config = BASE_GPT_CONFIG.copy()
    model_config["vocab_size"] = llm_tokenizer.get_vocab_size()
    
    print("üîß –°–æ–∑–¥–∞–Ω–∏–µ GPT –º–æ–¥–µ–ª–∏...")
    llm_model = GPT(model_config)
    
    # === –°–æ–∑–¥–∞–Ω–∏–µ –∞–¥–∞–ø—Ç–µ—Ä–∞ –º–æ–¥–µ–ª–∏ ===
    print("üîß –°–æ–∑–¥–∞–Ω–∏–µ –∞–¥–∞–ø—Ç–µ—Ä–∞ HuggingFace –¥–ª—è –º–æ–¥–µ–ª–∏...")
    hf_model = HFAdapter.from_llm_model(llm_model)
    print(f"‚úÖ –ê–¥–∞–ø—Ç–µ—Ä –º–æ–¥–µ–ª–∏ —Å–æ–∑–¥–∞–Ω")
    
    return hf_model, hf_tokenizer, llm_tokenizer, model_config, train_texts, val_texts


def test_hf_integration(hf_model, hf_tokenizer, llm_tokenizer):
    """
    –¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å HuggingFace –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏.
    
    Args:
        hf_model: –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        hf_tokenizer: –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        llm_tokenizer: –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    """
    print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å HuggingFace...")
    
    test_texts = ["–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏"]
    
    for text in test_texts:
        print(f"\nüî§ –¢–µ–∫—Å—Ç: '{text}'")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        hf_inputs = hf_tokenizer(text, return_tensors="pt")
        print(f"   HF —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: {hf_inputs['input_ids'].shape}")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        original_tokens = llm_tokenizer.encode(text)
        print(f"   –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: {len(original_tokens)} —Ç–æ–∫–µ–Ω–æ–≤")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º forward pass —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        try:
            with torch.no_grad():
                outputs = hf_model(**hf_inputs)
            print(f"   HF forward pass: —É—Å–ø–µ—à–Ω–æ (logits: {outputs.logits.shape})")
        except Exception as e:
            print(f"   ‚ùå HF forward pass: {e}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞."""
    # === –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ ===
    experiment_name = "–û–±—É—á–µ–Ω–∏–µ GPT —á–µ—Ä–µ–∑ HF Trainer (—Å hf-proxy)"
    experiment_config = {
        "model": "GPT —á–µ—Ä–µ–∑ HFAdapter",
        "tokenizer": "BPE —á–µ—Ä–µ–∑ HFTokenizerAdapter", 
        "trainer": "HuggingFace Trainer",
        "vocab_size": BPE_CONFIG["vocab_size"],
        "training_epochs": TRAINING_CONFIG["num_epochs"]
    }
    
    print_experiment_info(experiment_name, experiment_config)
    ensure_directories()
    logger = ExperimentLogger(experiment_name)
    
    try:
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
        hf_model, hf_tokenizer, llm_tokenizer, model_config, train_texts, val_texts = setup_hf_training()
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é
        test_hf_integration(hf_model, hf_tokenizer, llm_tokenizer)
        
        # === –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ HuggingFace ===
        print(f"\nüìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ HuggingFace...")
        
        from datasets import Dataset
        
        def tokenize_function(examples):
            """–§—É–Ω–∫—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–ª—è HF datasets."""
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            tokenized = hf_tokenizer(
                examples["text"],
                truncation=True,
                padding=False,
                max_length=model_config["max_position_embeddings"],
            )
            tokenized["labels"] = tokenized["input_ids"].copy()
            return tokenized
        
        # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã
        train_dataset = Dataset.from_dict({"text": train_texts})
        val_dataset = Dataset.from_dict({"text": val_texts})
        
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
        train_dataset = train_dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=train_dataset.column_names,
        )
        val_dataset = val_dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=val_dataset.column_names,
        )
        
        print(f"   Train –¥–∞—Ç–∞—Å–µ—Ç: {len(train_dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print(f"   Validation –¥–∞—Ç–∞—Å–µ—Ç: {len(val_dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        
        # === –ù–∞—Å—Ç—Ä–æ–π–∫–∞ HuggingFace Trainer ===
        print(f"\nüîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ HuggingFace Trainer...")
        
        from transformers import (
            Trainer, 
            TrainingArguments,
            DataCollatorForLanguageModeling
        )
        
        # Data collator –¥–ª—è —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=hf_tokenizer,
            mlm=False,
            pad_to_multiple_of=8,
        )
        
        # –ê—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è
        training_args = TrainingArguments(
            output_dir=PATHS["hf_model"],
            overwrite_output_dir=True,
            num_train_epochs=TRAINING_CONFIG["num_epochs"],
            per_device_train_batch_size=TRAINING_CONFIG["batch_size"],
            per_device_eval_batch_size=TRAINING_CONFIG["batch_size"],
            learning_rate=TRAINING_CONFIG["learning_rate"],
            warmup_steps=TRAINING_CONFIG["warmup_steps"],
            logging_dir="./logs",
            logging_steps=10,
            eval_steps=50,
            save_steps=100,
            eval_strategy="steps",
            save_strategy="steps",
            load_best_model_at_end=True,
            metric_for_best_model="loss",
            greater_is_better=False,
            dataloader_pin_memory=False,
            report_to=None,
        )
        
        # –°–æ–∑–¥–∞–µ–º Trainer
        trainer = Trainer(
            model=hf_model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            data_collator=data_collator,
        )
        
        print("‚úÖ HuggingFace Trainer –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
        
        # === –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è ===
        print(f"\nüéØ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ HuggingFace Trainer...")
        
        train_result = trainer.train()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        trainer.save_model()
        hf_tokenizer.save_pretrained(PATHS["hf_model"])
        
        print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
        print(f"üìä Final train loss: {train_result.metrics['train_loss']:.4f}")
        
        if "eval_loss" in train_result.metrics:
            print(f"üìä Final eval loss: {train_result.metrics['eval_loss']:.4f}")
        
        # === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ hf-proxy ===
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ hf-proxy...")
        
        from hf_proxy import convert_to_hf_format
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ HF —Ñ–æ—Ä–º–∞—Ç–µ
        hf_tokenizer_dir = PATHS["hf_tokenizer"]
        hf_tokenizer.save_pretrained(hf_tokenizer_dir)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ hf-proxy
        hf_proxy_dir = PATHS["hf_proxy_model"]
        HFAdapter.save_pretrained(hf_model, hf_proxy_dir, tokenizer=hf_tokenizer)
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ HF —Ñ–æ—Ä–º–∞—Ç–µ:")
        print(f"   - {PATHS['hf_model']}: —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π HF —Ñ–æ—Ä–º–∞—Ç")
        print(f"   - {hf_proxy_dir}: —á–µ—Ä–µ–∑ hf-proxy")
        print(f"   - {hf_tokenizer_dir}: —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ HF —Ñ–æ—Ä–º–∞—Ç–µ")
        
        # === –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ===
        print(f"\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è...")
        hf_model.eval()
        
        for prompt in TEST_PROMPTS[:3]:
            print(f"\nüî§ –ü—Ä–æ–º–ø—Ç: '{prompt}'")
            
            try:
                inputs = hf_tokenizer(prompt, return_tensors="pt")
                
                with torch.no_grad():
                    generated = hf_model.generate(
                        input_ids=inputs['input_ids'],
                        max_new_tokens=20,
                        do_sample=True,
                        temperature=0.8
                    )
                
                generated_text = hf_tokenizer.decode(generated[0], skip_special_tokens=True)
                print(f"üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç: '{generated_text}'")
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        
        # === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ===
        results = {
            "experiment": experiment_name,
            "model_config": model_config,
            "training_config": TRAINING_CONFIG,
            "final_loss": train_result.metrics.get('train_loss', 'N/A'),
            "eval_loss": train_result.metrics.get('eval_loss', 'N/A')
        }
        
        logger.save_logs("checkpoints/hf_integration_training_logs.json")
        
        print(f"\nüéâ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç —Å HF –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        print(f"\nüí° –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏:")
        print(f"   uv run python experiments/hf_integration/generate_with_hf_tools.py")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–µ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
