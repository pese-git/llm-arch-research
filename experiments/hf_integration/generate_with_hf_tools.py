#!/usr/bin/env python3
"""
Experiment: generate_with_hf_tools.py
Description: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –æ–±—É—á–µ–Ω–Ω–æ–π GPT –º–æ–¥–µ–ª—å—é —á–µ—Ä–µ–∑ HuggingFace –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç hf-proxy –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å HF —ç–∫–æ—Å–∏—Å—Ç–µ–º–æ–π.
"""

import torch
import os
import sys

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ shared –º–æ–¥—É–ª—è–º
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from hf_proxy import HFAdapter, HFTokenizerAdapter, create_hf_pipeline

from shared.configs import (
    TEST_PROMPTS, GENERATION_CONFIG, PATHS
)
from shared.data import (
    print_experiment_info, ensure_directories, ExperimentLogger
)


def load_hf_model_and_tokenizer() -> tuple:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ —Ñ–æ—Ä–º–∞—Ç–µ HuggingFace.
    
    Returns:
        tuple: (hf_model, hf_tokenizer, model_config)
    """
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é –º–æ–¥–µ–ª–∏
    model_path = "checkpoints/hf_simple_trained"
    tokenizer_path = "checkpoints/hf_simple_tokenizer"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
    if not os.path.exists(model_path):
        raise FileNotFoundError(
            f"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}\n"
            f"–°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å: uv run python experiments/hf_integration/simple_hf_training.py"
        )
    
    if not os.path.exists(tokenizer_path):
        raise FileNotFoundError(
            f"–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω: {tokenizer_path}"
        )
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    print("üîß –ó–∞–≥—Ä—É–∑–∫–∞ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    hf_tokenizer = HFTokenizerAdapter.from_pretrained(tokenizer_path)
    print(f"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω (vocab_size={hf_tokenizer.vocab_size})")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏
    import json
    config_path = os.path.join(model_path, "config.json")
    with open(config_path, 'r', encoding='utf-8') as f:
        model_config = json.load(f)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ HFAdapter —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
    print("üîß –ó–∞–≥—Ä—É–∑–∫–∞ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    model_bin_path = os.path.join(model_path, "pytorch_model.bin")
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ config.json
    from hf_proxy import HFAdapterConfig
    hf_config = HFAdapterConfig(
        vocab_size=model_config["vocab_size"],
        hidden_size=model_config["hidden_size"],
        num_hidden_layers=model_config["num_hidden_layers"],
        num_attention_heads=model_config["num_attention_heads"],
        max_position_embeddings=model_config["max_position_embeddings"],
        hidden_dropout_prob=model_config.get("hidden_dropout_prob", 0.1),
        attention_probs_dropout_prob=model_config.get("attention_probs_dropout_prob", 0.1),
    )
    
    hf_model = HFAdapter.from_pretrained(model_bin_path, hf_config=hf_config)
    hf_model.eval()
    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    return hf_model, hf_tokenizer, model_config


def test_hf_pipeline(hf_model, hf_tokenizer):
    """
    –¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Å–æ–∑–¥–∞–Ω–∏–µ HuggingFace pipeline.
    
    Args:
        hf_model: –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        hf_tokenizer: –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    """
    print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ HuggingFace pipeline...")
    
    try:
        # –°–æ–∑–¥–∞–µ–º pipeline
        pipe = create_hf_pipeline(
            hf_model,
            tokenizer=hf_tokenizer,
            device="cpu",
            max_length=50,
            do_sample=True,
            temperature=0.7
        )
        
        print("‚úÖ HuggingFace pipeline —Å–æ–∑–¥–∞–Ω")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º pipeline
        test_prompts = TEST_PROMPTS[:3]
        
        for prompt in test_prompts:
            print(f"\nüî§ –ü—Ä–æ–º–ø—Ç: '{prompt}'")
            
            try:
                result = pipe(prompt, max_new_tokens=20)
                print(f"üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç: {result[0]['generated_text']}")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ pipeline: {e}")
                
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è pipeline: {e}")


def generate_with_hf_model(hf_model, hf_tokenizer, prompt: str, config: dict) -> str:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å HF.
    
    Args:
        hf_model: –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        hf_tokenizer: –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        prompt: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        
    Returns:
        str: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    print(f"üî§ –ü—Ä–æ–º–ø—Ç: '{prompt}'")
    print(f"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: max_tokens={config['max_new_tokens']}, "
          f"temp={config['temperature']}, sample={config['do_sample']}")
    
    # –ö–æ–¥–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    inputs = hf_tokenizer(prompt, return_tensors="pt")
    
    print(f"üéØ –¢–æ–∫–µ–Ω—ã –ø—Ä–æ–º–ø—Ç–∞: {inputs['input_ids'].tolist()[0]}")
    print("üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ HF –∞–¥–∞–ø—Ç–µ—Ä...")
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
    with torch.no_grad():
        generated_ids = hf_model.generate(
            input_ids=inputs['input_ids'],
            max_new_tokens=config["max_new_tokens"],
            do_sample=config["do_sample"],
            temperature=config["temperature"],
            top_k=config["top_k"],
            top_p=config["top_p"]
        )
    
    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    generated_text = hf_tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    
    return generated_text


def test_different_hf_strategies(hf_model, hf_tokenizer, prompt: str):
    """
    –¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ HF –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å.
    
    Args:
        hf_model: –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        hf_tokenizer: –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        prompt: –¢–µ—Å—Ç–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç
    """
    print(f"\nüé≠ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ HF –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞: '{prompt}'")
    print("=" * 70)
    
    strategies = [
        {"name": "üéØ –ñ–∞–¥–Ω—ã–π –ø–æ–∏—Å–∫", "do_sample": False, "temperature": 1.0},
        {"name": "üé≤ –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–∞—è (temp=0.7)", "do_sample": True, "temperature": 0.7},
        {"name": "üî• –°–ª—É—á–∞–π–Ω–∞—è (temp=1.2)", "do_sample": True, "temperature": 1.2},
        {"name": "‚ùÑÔ∏è  –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è (temp=0.3)", "do_sample": True, "temperature": 0.3},
    ]
    
    for strategy in strategies:
        print(f"\n{strategy['name']}:")
        try:
            config = GENERATION_CONFIG.copy()
            config.update({
                "do_sample": strategy["do_sample"],
                "temperature": strategy["temperature"],
                "max_new_tokens": 20
            })
            
            generated = generate_with_hf_model(hf_model, hf_tokenizer, prompt, config)
            
            # –í—ã–¥–µ–ª—è–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —á–∞—Å—Ç—å
            generated_part = generated[len(prompt):]
            print(f"   üì§ –ü—Ä–æ–º–ø—Ç: '{prompt}'")
            print(f"   üéØ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: '{generated_part}'")
            print(f"   üìÑ –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç: '{generated}'")
            
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")


def analyze_hf_tokenization(hf_tokenizer, texts: list):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä.
    
    Args:
        hf_tokenizer: –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    """
    print(f"\nüîç –ê–Ω–∞–ª–∏–∑ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ HF –∞–¥–∞–ø—Ç–µ—Ä:")
    print("=" * 60)
    
    for i, text in enumerate(texts):
        print(f"\n–¢–µ–∫—Å—Ç {i+1}: '{text}'")
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ –∞–¥–∞–ø—Ç–µ—Ä
        inputs = hf_tokenizer(text, return_tensors="pt")
        tokens = inputs['input_ids'].tolist()[0]
        token_strings = hf_tokenizer.tokenize(text)
        
        print(f"   –¢–æ–∫–µ–Ω—ã (ID): {tokens}")
        print(f"   –¢–æ–∫–µ–Ω—ã (—Ç–µ–∫—Å—Ç): {token_strings}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(tokens)}")
        
        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ
        decoded = hf_tokenizer.decode(tokens)
        print(f"   –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π: '{decoded}'")
        
        if text == decoded:
            print(f"   ‚úÖ –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        else:
            print(f"   ‚ö†Ô∏è  –†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è")


def interactive_hf_generation(hf_model, hf_tokenizer):
    """
    –†–µ–∂–∏–º –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ HF –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å.
    
    Args:
        hf_model: –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        hf_tokenizer: –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    """
    print(f"\nüí¨ –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ HF (–¥–ª—è –≤—ã—Ö–æ–¥–∞ –≤–≤–µ–¥–∏—Ç–µ 'exit')")
    print("-" * 60)
    
    while True:
        try:
            user_input = input("\nüî§ –í–≤–µ–¥–∏—Ç–µ –ø—Ä–æ–º–ø—Ç: ").strip()
            
            if user_input.lower() in ['exit', 'quit', '–≤—ã—Ö–æ–¥']:
                break
                
            if not user_input:
                continue
            
            # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            try:
                max_tokens = int(input("üìè –ú–∞–∫—Å. —Ç–æ–∫–µ–Ω–æ–≤ [50]: ") or "50")
                temperature = float(input("üå°Ô∏è  –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ [0.7]: ") or "0.7")
                do_sample_input = input("üé≤ –°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (y/n) [y]: ").lower()
                do_sample = do_sample_input != 'n'
            except:
                max_tokens = 50
                temperature = 0.7
                do_sample = True
                print("‚ö†Ô∏è  –ò—Å–ø–æ–ª—å–∑—É—é –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
            
            config = GENERATION_CONFIG.copy()
            config.update({
                "max_new_tokens": max_tokens,
                "temperature": temperature,
                "do_sample": do_sample
            })
            
            generated = generate_with_hf_model(hf_model, hf_tokenizer, user_input, config)
            
            generated_part = generated[len(user_input):]
            print(f"\nüéØ –†–µ–∑—É–ª—å—Ç–∞—Ç:")
            print(f"   üì§ –ü—Ä–æ–º–ø—Ç: '{user_input}'")
            print(f"   üéØ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: '{generated_part}'")
            print(f"   üìÑ –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç: '{generated}'")
            
        except KeyboardInterrupt:
            print("\nüëã –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã...")
            break
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞."""
    # === –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ ===
    experiment_name = "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ HF –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã (—Å hf-proxy)"
    experiment_config = {
        "model": "GPT —á–µ—Ä–µ–∑ HFAdapter",
        "tokenizer": "BPE —á–µ—Ä–µ–∑ HFTokenizerAdapter",
        "–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã": "HuggingFace pipeline & –≥–µ–Ω–µ—Ä–∞—Ü–∏—è",
        "—Å—Ç—Ä–∞—Ç–µ–≥–∏—è": "–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å HF —ç–∫–æ—Å–∏—Å—Ç–µ–º–æ–π"
    }
    
    print_experiment_info(experiment_name, experiment_config)
    ensure_directories()
    logger = ExperimentLogger(experiment_name)
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ HF —Ñ–æ—Ä–º–∞—Ç–µ
        hf_model, hf_tokenizer, model_config = load_hf_model_and_tokenizer()
        
        # === –ê–Ω–∞–ª–∏–∑ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ ===
        analysis_texts = [
            "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç",
            "–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏", 
            "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"
        ]
        analyze_hf_tokenization(hf_tokenizer, analysis_texts)
        
        # === –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ HF pipeline ===
        test_hf_pipeline(hf_model, hf_tokenizer)
        
        # === –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å —Ä–∞–∑–Ω—ã–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏ ===
        print(f"\nüéØ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ HF –∞–¥–∞–ø—Ç–µ—Ä")
        print("=" * 60)
        
        for i, prompt in enumerate(TEST_PROMPTS):
            print(f"\nüìù –ü—Ä–∏–º–µ—Ä {i+1}/{len(TEST_PROMPTS)}")
            print("-" * 40)
            
            try:
                generated = generate_with_hf_model(hf_model, hf_tokenizer, prompt, GENERATION_CONFIG)
                
                # –í—ã–¥–µ–ª—è–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —á–∞—Å—Ç—å
                generated_part = generated[len(prompt):]
                
                print(f"üì§ –ü—Ä–æ–º–ø—Ç: '{prompt}'")
                print(f"üéØ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: '{generated_part}'")
                print(f"üìÑ –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç: '{generated}'")
                print(f"üìè –î–ª–∏–Ω–∞: {len(generated)} —Å–∏–º–≤–æ–ª–æ–≤")
                
                # –õ–æ–≥–∏—Ä—É–µ–º —É—Å–ø–µ—à–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
                logger.log_metric(f"hf_generation_length_{i}", len(generated))
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
                continue
        
        # === –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ===
        test_prompt = "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π"
        test_different_hf_strategies(hf_model, hf_tokenizer, test_prompt)
        
        # === –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è ===
        interactive_hf_generation(hf_model, hf_tokenizer)
        
        # === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ===
        logger.save_logs("checkpoints/hf_integration_generation_logs.json")
        
        print(f"\nüéâ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç —Å HF –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        print(f"\nüìö –î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è:")
        print(f"   ‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –≤ HF —Ñ–æ—Ä–º–∞—Ç–µ")
        print(f"   ‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ HF pipeline")
        print(f"   ‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ HF –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã")
        print(f"   ‚úÖ –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å HF —ç–∫–æ—Å–∏—Å—Ç–µ–º–æ–π")
        
    except FileNotFoundError as e:
        print(f"‚ùå {e}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–µ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
