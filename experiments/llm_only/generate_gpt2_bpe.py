#!/usr/bin/env python3
"""
Experiment: generate_gpt_bpe.py
Description: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –æ–±—É—á–µ–Ω–Ω–æ–π GPT –º–æ–¥–µ–ª—å—é —Å BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ –±–∏–±–ª–∏–æ—Ç–µ–∫—É llm –±–µ–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –æ—Ç HuggingFace.
"""

import torch
import os
import sys

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ shared –º–æ–¥—É–ª—è–º
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from llm.models.gpt import GPT2
from llm.tokenizers import BPETokenizer

from shared.configs import BASE_GPT_CONFIG, TEST_PROMPTS, GENERATION_CONFIG, PATHS
from shared.data import print_experiment_info, ensure_directories, ExperimentLogger


def load_model_and_tokenizer() -> tuple:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä.

    Returns:
        tuple: (–º–æ–¥–µ–ª—å, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è)
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
    if not os.path.exists(PATHS["gpt_bpe_model"]):
        raise FileNotFoundError(
            f"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {PATHS['gpt_bpe_model']}\n"
            f"–°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å: uv run python experiments/llm_only/train_gpt_bpe.py"
        )

    if not os.path.exists(PATHS["bpe_tokenizer"]):
        raise FileNotFoundError(f"–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω: {PATHS['bpe_tokenizer']}")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏
    import json

    with open(PATHS["gpt_bpe_config"], "r", encoding="utf-8") as f:
        model_config = json.load(f)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    print("üîß –ó–∞–≥—Ä—É–∑–∫–∞ BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    tokenizer = BPETokenizer.load(PATHS["bpe_tokenizer"])
    print(f"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω (vocab_size={tokenizer.get_vocab_size()})")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    print("üîß –ó–∞–≥—Ä—É–∑–∫–∞ GPT2 –º–æ–¥–µ–ª–∏...")
    model = GPT2(model_config)
    model.load_state_dict(torch.load(PATHS["gpt_bpe_model"], map_location="cpu"))
    model.eval()
    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

    return model, tokenizer, model_config


def generate_text(
    model: GPT2, tokenizer: BPETokenizer, prompt: str, config: dict
) -> str:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–º–ø—Ç–∞.

    Args:
        model: –û–±—É—á–µ–Ω–Ω–∞—è GPT –º–æ–¥–µ–ª—å
        tokenizer: BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        prompt: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

    Returns:
        str: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    print(f"üî§ –ü—Ä–æ–º–ø—Ç: '{prompt}'")
    print(
        f"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: max_tokens={config['max_new_tokens']}, "
        f"temp={config['temperature']}, sample={config['do_sample']}"
    )

    # –ö–æ–¥–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
    input_ids = tokenizer.encode(prompt, add_special_tokens=False)
    input_tensor = torch.tensor([input_ids], dtype=torch.long)

    print(f"üéØ –¢–æ–∫–µ–Ω—ã –ø—Ä–æ–º–ø—Ç–∞: {input_ids}")
    print(f"üéØ –¢–æ–∫–µ–Ω—ã (—Ç–µ–∫—Å—Ç): {tokenizer.tokenize(prompt)}")
    print("üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è...")

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
    with torch.no_grad():
        generated_ids = model.generate(
            x=input_tensor,
            max_new_tokens=config["max_new_tokens"],
            do_sample=config["do_sample"],
            temperature=config["temperature"],
            top_k=config["top_k"],
            top_p=config["top_p"],
        )

    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    generated_text = tokenizer.decode(generated_ids[0].tolist())

    return generated_text


def test_different_strategies(model: GPT2, tokenizer: BPETokenizer, prompt: str):
    """
    –¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞ –æ–¥–Ω–æ–º –ø—Ä–æ–º–ø—Ç–µ.

    Args:
        model: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        tokenizer: BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        prompt: –¢–µ—Å—Ç–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç
    """
    print(f"\nüé≠ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞: '{prompt}'")
    print("=" * 60)

    strategies = [
        {"name": "üéØ –ñ–∞–¥–Ω—ã–π –ø–æ–∏—Å–∫", "do_sample": False, "temperature": 1.0},
        {"name": "üé≤ –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–∞—è (temp=0.7)", "do_sample": True, "temperature": 0.7},
        {"name": "üî• –°–ª—É—á–∞–π–Ω–∞—è (temp=1.2)", "do_sample": True, "temperature": 1.2},
        {
            "name": "‚ùÑÔ∏è  –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è (temp=0.3)",
            "do_sample": True,
            "temperature": 0.3,
        },
    ]

    for strategy in strategies:
        print(f"\n{strategy['name']}:")
        try:
            config = GENERATION_CONFIG.copy()
            config.update(
                {
                    "do_sample": strategy["do_sample"],
                    "temperature": strategy["temperature"],
                    "max_new_tokens": 20,
                }
            )

            generated = generate_text(model, tokenizer, prompt, config)

            # –í—ã–¥–µ–ª—è–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —á–∞—Å—Ç—å
            generated_part = generated[len(prompt) :]
            print(f"   üì§ –ü—Ä–æ–º–ø—Ç: '{prompt}'")
            print(f"   üéØ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: '{generated_part}'")
            print(f"   üìÑ –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç: '{generated}'")

        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")


def analyze_tokenization(tokenizer: BPETokenizer, texts: list):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤.

    Args:
        tokenizer: BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    """
    print(f"\nüîç –ê–Ω–∞–ª–∏–∑ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ BPE:")
    print("=" * 50)

    for i, text in enumerate(texts):
        print(f"\n–¢–µ–∫—Å—Ç {i+1}: '{text}'")

        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        tokens = tokenizer.encode(text, add_special_tokens=False)
        token_strings = tokenizer.tokenize(text)

        print(f"   –¢–æ–∫–µ–Ω—ã (ID): {tokens}")
        print(f"   –¢–æ–∫–µ–Ω—ã (—Ç–µ–∫—Å—Ç): {token_strings}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(tokens)}")
        print(f"   –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ ‚Üí {len(tokens)} —Ç–æ–∫–µ–Ω–æ–≤")

        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ
        decoded = tokenizer.decode(tokens)
        if text == decoded:
            print(f"   ‚úÖ –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        else:
            print(f"   ‚ö†Ô∏è  –†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è: '{decoded}'")


def interactive_generation(model: GPT2, tokenizer: BPETokenizer):
    """
    –†–µ–∂–∏–º –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.

    Args:
        model: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        tokenizer: BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    """
    print(f"\nüí¨ –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è (–¥–ª—è –≤—ã—Ö–æ–¥–∞ –≤–≤–µ–¥–∏—Ç–µ 'exit')")
    print("-" * 50)

    while True:
        try:
            user_input = input("\nüî§ –í–≤–µ–¥–∏—Ç–µ –ø—Ä–æ–º–ø—Ç: ").strip()

            if user_input.lower() in ["exit", "quit", "–≤—ã—Ö–æ–¥"]:
                break

            if not user_input:
                continue

            # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            try:
                max_tokens = int(input("üìè –ú–∞–∫—Å. —Ç–æ–∫–µ–Ω–æ–≤ [50]: ") or "50")
                temperature = float(input("üå°Ô∏è  –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ [0.7]: ") or "0.7")
                do_sample_input = input("üé≤ –°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (y/n) [y]: ").lower()
                do_sample = do_sample_input != "n"
            except:
                max_tokens = 50
                temperature = 0.7
                do_sample = True
                print("‚ö†Ô∏è  –ò—Å–ø–æ–ª—å–∑—É—é –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")

            config = GENERATION_CONFIG.copy()
            config.update(
                {
                    "max_new_tokens": max_tokens,
                    "temperature": temperature,
                    "do_sample": do_sample,
                }
            )

            generated = generate_text(model, tokenizer, user_input, config)

            generated_part = generated[len(user_input) :]
            print(f"\nüéØ –†–µ–∑—É–ª—å—Ç–∞—Ç:")
            print(f"   üì§ –ü—Ä–æ–º–ø—Ç: '{user_input}'")
            print(f"   üéØ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: '{generated_part}'")
            print(f"   üìÑ –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç: '{generated}'")

        except KeyboardInterrupt:
            print("\nüëã –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã...")
            break
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞."""
    # === –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ ===
    experiment_name = "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ GPT2 + BPE (—Ç–æ–ª—å–∫–æ llm)"
    experiment_config = {
        "model": "GPT2 —Å BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º",
        "—Å—Ç—Ä–∞—Ç–µ–≥–∏—è": "–∞–≤—Ç–æ–Ω–æ–º–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è",
        "–≤—Ö–æ–¥": "–ø—Ä–æ–º–ø—Ç—ã",
        "–≤—ã—Ö–æ–¥": "—Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç",
    }

    print_experiment_info(experiment_name, experiment_config)
    ensure_directories()
    logger = ExperimentLogger(experiment_name)

    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        model, tokenizer, model_config = load_model_and_tokenizer()

        # === –ê–Ω–∞–ª–∏–∑ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ ===
        analysis_texts = [
            "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç",
            "–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏",
            "–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ",
        ]
        analyze_tokenization(tokenizer, analysis_texts)

        # === –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å —Ä–∞–∑–Ω—ã–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏ ===
        print(f"\nüéØ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å —Ä–∞–∑–Ω—ã–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏")
        print("=" * 60)

        for i, prompt in enumerate(TEST_PROMPTS):
            print(f"\nüìù –ü—Ä–∏–º–µ—Ä {i+1}/{len(TEST_PROMPTS)}")
            print("-" * 40)

            try:
                generated = generate_text(model, tokenizer, prompt, GENERATION_CONFIG)

                # –í—ã–¥–µ–ª—è–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —á–∞—Å—Ç—å
                generated_part = generated[len(prompt) :]

                print(f"üì§ –ü—Ä–æ–º–ø—Ç: '{prompt}'")
                print(f"üéØ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: '{generated_part}'")
                print(f"üìÑ –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç: '{generated}'")
                print(f"üìè –î–ª–∏–Ω–∞: {len(generated)} —Å–∏–º–≤–æ–ª–æ–≤")

                # –õ–æ–≥–∏—Ä—É–µ–º —É—Å–ø–µ—à–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
                logger.log_metric(f"generation_length_{i}", len(generated))

            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
                continue

        # === –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ===
        test_prompt = "–ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π"
        test_different_strategies(model, tokenizer, test_prompt)

        # === –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è ===
        interactive_generation(model, tokenizer)

        # === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ===
        logger.save_logs("checkpoints/llm_only_generation_logs.json")

        print(f"\nüéâ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")

    except FileNotFoundError as e:
        print(f"‚ùå {e}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–µ: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
