#!/usr/bin/env python3
"""
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ LLM.
–ü–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã–±–∏—Ä–∞—Ç—å —Ç–∏–ø –º–æ–¥–µ–ª–∏ –∏ –¥–µ–π—Å—Ç–≤–∏–µ —á–µ—Ä–µ–∑ –∞—Ä–≥—É–º–µ–Ω—Ç—ã,
–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥–∞–≤–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–º JSON-–∫–æ–Ω—Ñ–∏–≥–æ–º.
"""

import argparse
import json
import os
import sys

import torch

# –î–æ–±–∞–≤–ª—è–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é shared —Å—Ä–µ–¥–∏ –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º—ã—Ö
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from llm.tokenizers import BPETokenizer
from llm.datasets.text_dataset import TextDataset
from llm.training.trainer import Trainer

from shared.data import (
    print_experiment_info,
    ensure_directories,
    load_training_data,
    ExperimentLogger,
)


def load_config(config_path):
    with open(config_path, "r", encoding="utf-8") as f:
        return json.load(f)


def load_model_class(model_name):
    if model_name.lower() == 'gpt':
        from llm.models.gpt import GPT
        return GPT
    elif model_name.lower() == 'gpt2':
        from llm.models.gpt import GPT2
        return GPT2
    elif model_name.lower() == 'llama':
        from llm.models.llama import Llama
        return Llama
    elif model_name.lower() == 'mistral':
        from llm.models.mistral import Mistral
        return Mistral
    elif model_name.lower() == 'mixtral':
        from llm.models.mixtral import Mixtral
        return Mixtral
    elif model_name.lower() == 'gemma':
        from llm.models.gemma import Gemma
        return Gemma
    else:
        raise ValueError(f"–ú–æ–¥–µ–ª—å '{model_name}' –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è.")


def main():
    parser = argparse.ArgumentParser(description='–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è/–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ LLM.')
    parser.add_argument('--model', '-m', type=str, required=True, help='–ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (gpt, gpt2, llama –∏ —Ç.–¥.).')
    parser.add_argument('--action', '-a', type=str, required=True, choices=['train', 'generate'], help='–î–µ–π—Å—Ç–≤–∏–µ: train –∏–ª–∏ generate.')
    parser.add_argument('--config', '-c', type=str, required=True, help='–ü—É—Ç—å –∫ JSON-–∫–æ–Ω—Ñ–∏–≥—É —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.')
    args = parser.parse_args()

    config = load_config(args.config)
    ModelClass = load_model_class(args.model)
    logger = ExperimentLogger(f"{args.action}_{args.model}")

    print_experiment_info(f"–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç {args.action} {args.model}", config)
    ensure_directories()

    # ==== –û–±—É—á–µ–Ω–∏–µ ====
    if args.action == 'train':
        train_texts, val_texts = load_training_data()
        # --- –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä ---
        if os.path.exists(config["bpe_tokenizer"]):
            print("üìù –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
            tokenizer = BPETokenizer.load(config["bpe_tokenizer"])
            print(f"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω (vocab_size={tokenizer.get_vocab_size()})")
        else:
            print("üîß –û–±—É—á–µ–Ω–∏–µ BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
            tokenizer = BPETokenizer()
            tokenizer.train(
                texts=train_texts,
                vocab_size=config["bpe_vocab_size"],
                special_tokens=config["bpe_special_tokens"]
            )
            os.makedirs(os.path.dirname(config["bpe_tokenizer"]), exist_ok=True)
            tokenizer.save(config["bpe_tokenizer"])
            print(f"‚úÖ BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –æ–±—É—á–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {config['bpe_tokenizer']}")

        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (–±–∞–∑–æ–≤–æ)
        for test_text in config.get("test_prompts", ["–¢–µ—Å—Ç"]):
            encoded = tokenizer.encode(test_text)
            decoded = tokenizer.decode(encoded)
            print(f"[TEST TOK] '{test_text}' ‚Üí {encoded} ‚Üí '{decoded}'")

        # --- –ú–æ–¥–µ–ª—å ---
        model_config = config["model_config"]
        model_config["vocab_size"] = tokenizer.get_vocab_size()
        model = ModelClass(model_config)

        # --- –î–∞—Ç–∞—Å–µ—Ç ---
        train_dataset = TextDataset(
            train_texts,
            tokenizer,
            block_size=model_config["max_position_embeddings"]
        )
        print(f"   –†–∞–∑–º–µ—Ä train –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(train_dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤")

        # --- Trainer ---
        training = config["training"]
        trainer = Trainer(
            model=model,
            train_dataset=train_dataset,
            lr=training["learning_rate"],
            batch_size=training["batch_size"],
            num_epochs=training["num_epochs"],
            warmup_steps=training.get("warmup_steps", 0),
        )
        trainer.train()

        # --- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ ---
        os.makedirs(os.path.dirname(config["model_weights"]), exist_ok=True)
        torch.save(model.state_dict(), config["model_weights"])
        with open(config["model_config_path"], "w", encoding="utf-8") as f:
            json.dump(model_config, f, indent=2, ensure_ascii=False)
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {config['model_weights']}")

        logger.save_logs(config.get("log_path", "checkpoints/llm_only_training_logs.json"))

    # ==== –ì–µ–Ω–µ—Ä–∞—Ü–∏—è ====
    elif args.action == 'generate':
        # --- –ó–∞–≥—Ä—É–∑–∫–∞ ---
        if not os.path.exists(config["model_weights"]):
            raise FileNotFoundError(f"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {config['model_weights']}")
        if not os.path.exists(config["bpe_tokenizer"]):
            raise FileNotFoundError(f"–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω: {config['bpe_tokenizer']}")
        with open(config["model_config_path"], "r", encoding="utf-8") as f:
            model_config = json.load(f)
        tokenizer = BPETokenizer.load(config["bpe_tokenizer"])
        model = ModelClass(model_config)
        model.load_state_dict(torch.load(config["model_weights"], map_location="cpu"))
        model.eval()

        def generate(prompt, gen_cfg):
            print(f"–ü—Ä–æ–º–ø—Ç: {prompt}")
            input_ids = tokenizer.encode(prompt, add_special_tokens=False)
            input_tensor = torch.tensor([input_ids], dtype=torch.long)
            with torch.no_grad():
                generated_ids = model.generate(
                    x=input_tensor,
                    max_new_tokens=gen_cfg["max_new_tokens"],
                    do_sample=gen_cfg["do_sample"],
                    temperature=gen_cfg["temperature"],
                    top_k=gen_cfg.get("top_k"),
                    top_p=gen_cfg.get("top_p"),
                )
            return tokenizer.decode(generated_ids[0].tolist())

        prompts = config.get("test_prompts", ["–¢–µ—Å—Ç–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç"])
        gen_cfg = config.get("generation", {
            "max_new_tokens": 50,
            "temperature": 0.7,
            "do_sample": True,
            "top_k": None,
            "top_p": None
        })
        for prompt in prompts:
            generated = generate(prompt, gen_cfg)
            print(f"\n[RESULT] Prompt: '{prompt}'\n---\n{generated}\n{'='*60}")

        logger.save_logs(config.get("log_path", "checkpoints/llm_only_generation_logs.json"))

if __name__ == "__main__":
    main()
