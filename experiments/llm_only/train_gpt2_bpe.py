#!/usr/bin/env python3
"""
Experiment: train_gpt_bpe.py
Description: –û–±—É—á–µ–Ω–∏–µ GPT –º–æ–¥–µ–ª–∏ —Å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ –±–∏–±–ª–∏–æ—Ç–µ–∫—É llm –±–µ–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –æ—Ç HuggingFace.
"""

import torch
import os
import sys

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ shared –º–æ–¥—É–ª—è–º
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from llm.models.gpt import GPT2
from llm.tokenizers import BPETokenizer
from llm.training.dataset import TextDataset
from llm.training.trainer import Trainer

from shared.configs import (
    TRAIN_TEXTS, BASE_GPT_CONFIG, BPE_CONFIG, 
    TRAINING_CONFIG, PATHS, TEST_PROMPTS
)
from shared.data import (
    load_training_data, ensure_directories, 
    print_experiment_info, ExperimentLogger
)


def train_bpe_tokenizer(texts: list, config: dict) -> BPETokenizer:
    """
    –û–±—É—á–∞–µ—Ç BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ —Ç–µ–∫—Å—Ç–∞—Ö.
    
    Args:
        texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        
    Returns:
        BPETokenizer: –û–±—É—á–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    """
    print("üîß –û–±—É—á–µ–Ω–∏–µ BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    
    tokenizer = BPETokenizer()
    tokenizer.train(
        texts=texts,
        vocab_size=config["vocab_size"],
        special_tokens=config["special_tokens"]
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    os.makedirs(os.path.dirname(PATHS["bpe_tokenizer"]), exist_ok=True)
    tokenizer.save(PATHS["bpe_tokenizer"])
    
    print(f"‚úÖ BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –æ–±—É—á–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {PATHS['bpe_tokenizer']}")
    print(f"üìä –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {tokenizer.get_vocab_size()}")
    
    return tokenizer


def test_tokenizer(tokenizer: BPETokenizer, texts: list):
    """
    –¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö.
    
    Args:
        tokenizer: –û–±—É—á–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        texts: –°–ø–∏—Å–æ–∫ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
    """
    print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞:")
    
    for i, text in enumerate(texts[:3]):
        print(f"\n–ü—Ä–∏–º–µ—Ä {i+1}:")
        print(f"   –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: '{text}'")
        
        # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        tokens = tokenizer.encode(text)
        token_strings = tokenizer.tokenize(text)
        
        print(f"   –¢–æ–∫–µ–Ω—ã (ID): {tokens}")
        print(f"   –¢–æ–∫–µ–Ω—ã (—Ç–µ–∫—Å—Ç): {token_strings}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(tokens)}")
        
        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        decoded = tokenizer.decode(tokens)
        print(f"   –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π: '{decoded}'")
        
        if text == decoded:
            print("   ‚úÖ –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ/–¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        else:
            print("   ‚ö†Ô∏è  –ù–µ–±–æ–ª—å—à–∏–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞."""
    # === –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ ===
    experiment_name = "–û–±—É—á–µ–Ω–∏–µ GPT2 —Å BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º (—Ç–æ–ª—å–∫–æ llm)"
    experiment_config = {
        "model": "GPT2",
        "tokenizer": "BPE", 
        "vocab_size": BPE_CONFIG["vocab_size"],
        "training_epochs": TRAINING_CONFIG["num_epochs"],
        "batch_size": TRAINING_CONFIG["batch_size"],
        "learning_rate": TRAINING_CONFIG["learning_rate"]
    }
    
    print_experiment_info(experiment_name, experiment_config)
    ensure_directories()
    logger = ExperimentLogger(experiment_name)
    
    try:
        # === –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===
        train_texts, val_texts = load_training_data()
        print(f"üìä –î–∞–Ω–Ω—ã–µ: {len(train_texts)} train, {len(val_texts)} validation")
        
        # === –û–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ ===
        if os.path.exists(PATHS["bpe_tokenizer"]):
            print("üìù –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
            tokenizer = BPETokenizer.load(PATHS["bpe_tokenizer"])
            print(f"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω (vocab_size={tokenizer.get_vocab_size()})")
        else:
            tokenizer = train_bpe_tokenizer(TRAIN_TEXTS, BPE_CONFIG)
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        test_tokenizer(tokenizer, TEST_PROMPTS[:3])
        
        # === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ ===
        model_config = BASE_GPT_CONFIG.copy()
        model_config["vocab_size"] = tokenizer.get_vocab_size()
        
        print(f"\nüîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GPT2 –º–æ–¥–µ–ª–∏...")
        print(f"   –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {model_config['vocab_size']}")
        print(f"   –†–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {model_config['embed_dim']}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤: {model_config['num_layers']}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è: {model_config['num_heads']}")
        
        model = GPT2(model_config)
        
        # === –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ ===
        print(f"\nüìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        train_dataset = TextDataset(
            train_texts, 
            tokenizer, 
            block_size=model_config["max_position_embeddings"]
        )
        print(f"   –†–∞–∑–º–µ—Ä train –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(train_dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        
        # === –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ ===
        print(f"\nüéØ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è GPT2 –º–æ–¥–µ–ª–∏...")
        
        trainer = Trainer(
            model=model,
            train_dataset=train_dataset,
            lr=TRAINING_CONFIG["learning_rate"],
            batch_size=TRAINING_CONFIG["batch_size"],
            num_epochs=TRAINING_CONFIG["num_epochs"],
            warmup_steps=TRAINING_CONFIG["warmup_steps"]
        )
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
        trainer.train()
        
        # === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ ===
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        os.makedirs(os.path.dirname(PATHS["gpt_bpe_model"]), exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        torch.save(model.state_dict(), PATHS["gpt_bpe_model"])
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        import json
        with open(PATHS["gpt_bpe_config"], 'w', encoding='utf-8') as f:
            json.dump(model_config, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞:")
        print(f"   - {PATHS['gpt_bpe_model']}: –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏")
        print(f"   - {PATHS['gpt_bpe_config']}: –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏")
        print(f"   - {PATHS['bpe_tokenizer']}: —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä")
        
        # === –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ===
        print(f"\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞...")
        model.eval()
        
        for prompt in TEST_PROMPTS[:3]:
            print(f"\nüî§ –ü—Ä–æ–º–ø—Ç: '{prompt}'")
            
            try:
                # –ö–æ–¥–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
                input_ids = tokenizer.encode(prompt, add_special_tokens=False)
                input_tensor = torch.tensor([input_ids], dtype=torch.long)
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
                with torch.no_grad():
                    generated_ids = model.generate(
                        x=input_tensor,
                        max_new_tokens=20,
                        do_sample=True,
                        temperature=0.8
                    )
                
                # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                generated_text = tokenizer.decode(generated_ids[0].tolist())
                generated_part = generated_text[len(prompt):]
                
                print(f"üéØ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: '{generated_part}'")
                print(f"üìÑ –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç: '{generated_text}'")
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        
        # === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ===
        results = {
            "experiment": experiment_name,
            "model_config": model_config,
            "training_config": TRAINING_CONFIG,
            "tokenizer_vocab_size": tokenizer.get_vocab_size(),
            "final_loss": "—Å–º. –ª–æ–≥–∏ –æ–±—É—á–µ–Ω–∏—è"  # –í —Ä–µ–∞–ª—å–Ω–æ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–µ –º–æ–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å final loss
        }
        
        logger.save_logs("checkpoints/llm_only_training_logs.json")
        
        print(f"\nüéâ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        print(f"\nüí° –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏:")
        print(f"   uv run python experiments/llm_only/generate_gpt_bpe.py")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–µ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
