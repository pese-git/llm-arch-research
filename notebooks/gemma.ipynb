{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1636810a",
   "metadata": {},
   "source": [
    "# Gemma\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://ucarecdn.com/7b36315f-fc63-4f57-8344-12f1a993376f/\" alt=\"arch\" width=\"1000\" height=\"165\">\n",
    "</p>\n",
    "\n",
    "Gemma 1 –≤—ã—à–ª–∞ –≤ —Ñ–µ–≤—Ä–∞–ª–µ 2024 –≥–æ–¥–∞.\n",
    "\n",
    "–ü–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –º–æ–¥–µ–ª—å –±–æ–ª—å—à–µ –≤—Å–µ–≥–æ –ø–æ—Ö–æ–∂–∞ –Ω–∞ Llama'—É. –°–æ–¥–µ—Ä–∂–∏—Ç —É–∂–µ –∑–Ω–∞–∫–æ–º—ã–µ –Ω–∞–º RoPE –∏ RMSNorm. –ù–æ –µ—Å—Ç—å –∏ –Ω–æ–≤–∏–Ω–∫–∏:\n",
    "\n",
    "* **Multi-Query Attention (MQA)** ‚Äî –∫—Ä–∞–π–Ω–µ —ç–∫–æ–Ω–æ–º–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è.\n",
    "* **GeGLU** ‚Äî –≥–∏–±—Ä–∏–¥–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏. –ü–æ—á—Ç–∏ –∫–ª–æ–Ω SwiGLU :)\n",
    "\n",
    "–û–±–µ –¥–æ–≤–æ–ª—å–Ω–æ –ª–µ–≥–∫–∏–µ –¥–ª—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è, –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–æ—à–ª—ã–º–∏ –Ω–æ–≤–∏–Ω–∫–∞–º–∏ :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea30169",
   "metadata": {},
   "source": [
    "# Multi-Query Attention\n",
    "\n",
    "–ü–æ —Å–≤–æ–µ–π —Å—É—Ç–∏, Multi-Query Attention (MQA) ‚Äî —ç—Ç–æ —á–∞—Å—Ç–Ω—ã–π —Å–ª—É—á–∞–π Grouped Query Attention (GQA), –∫–æ—Ç–æ—Ä—ã–π –º—ã —Ä–µ–∞–ª–∏–∑–æ–≤–∞–ª–∏ –≤ —É—Ä–æ–∫–µ –ø—Ä–æ Mistral.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://ucarecdn.com/d1b0b294-9c02-4de1-9074-ae8ebfe96f6e/\" alt=\"mqa\" width=\"1000\" height=\"217\">\n",
    "</p>\n",
    "\n",
    "–í GQA –Ω–∞ –∫–∞–∂–¥—É—é –≥–æ–ª–æ–≤—É –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è –æ–¥–∏–Ω –≤–µ–∫—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–∞ (query). –ü—Ä–∏ —ç—Ç–æ–º –∫–∞–∂–¥—ã–π –≤–µ–∫—Ç–æ—Ä –∫–ª—é—á–∞ (key) –∏ –∑–Ω–∞—á–µ–Ω–∏—è (value) –æ–±—Å–ª—É–∂–∏–≤–∞–µ—Ç ( n )-–≥–æ–ª–æ–≤.\n",
    "–¢–∞–∫ –≤–æ—Ç, –≤ MQA –Ω–∞ –≤—Å–µ –≥–æ–ª–æ–≤—ã (–≤ –æ–¥–Ω–æ–º –±–ª–æ–∫–µ –¥–µ–∫–æ–¥–µ—Ä–∞) –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è –≤—Å–µ–≥–æ –ø–æ –æ–¥–Ω–æ–º—É –≤–µ–∫—Ç–æ—Ä—É –∫–ª—é—á–∞ (key) –∏ –æ–¥–Ω–æ–º—É –≤–µ–∫—Ç–æ—Ä—É –∑–Ω–∞—á–µ–Ω–∏—è (value). –≠—Ç–æ —Ç–∞–∫–∞—è —Ä–∞–¥–∏–∫–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ —ç–∫–æ–Ω–æ–º–∏–∏ :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539550fe",
   "metadata": {},
   "source": [
    "**Multi-Query Attention (—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6be61c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class RoPE(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size: int, max_seq_len: int, base: int = 10_000):\n",
    "        super().__init__()\n",
    "        assert head_size % 2 == 0, \"head_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —á–µ—Ç–Ω—ã–º\"\n",
    "\n",
    "        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç: Œ∏_i = base^(-2i/d) –¥–ª—è i ‚àà [0, d/2-1]\n",
    "        freqs = 1.0 / (base ** (2 * torch.arange(head_size // 2).float() / head_size))\n",
    "\n",
    "        # –ü–æ–∑–∏—Ü–∏–∏ –æ—Ç 0 –¥–æ max_seq_len-1\n",
    "        positions = torch.arange(max_seq_len).float()\n",
    "\n",
    "        # –í–Ω–µ—à–Ω–µ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ: m * Œ∏_i –¥–ª—è –≤—Å–µ—Ö –ø–æ–∑–∏—Ü–∏–π –∏ —á–∞—Å—Ç–æ—Ç\n",
    "        freq_matrix = positions.unsqueeze(1) * freqs.unsqueeze(0)\n",
    "\n",
    "        # –ü—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü –∫–æ—Å–∏–Ω—É—Å–æ–≤ –∏ —Å–∏–Ω—É—Å–æ–≤\n",
    "        self.register_buffer(\"cos_matrix\", torch.cos(freq_matrix))\n",
    "        self.register_buffer(\"sin_matrix\", torch.sin(freq_matrix))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: # [batch_size √ó seq_len √ó head_size] [batch_size √ó num_heads √ó seq_len √ó head_size]\n",
    "        batch_size, num_heads, seq_len, head_size = x.shape\n",
    "\n",
    "        # –ë–µ—Ä–µ–º –Ω—É–∂–Ω—É—é —á–∞—Å—Ç—å –º–∞—Ç—Ä–∏—Ü –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ —Ç–∏–ø—É x\n",
    "        cos = self.cos_matrix[:seq_len].to(x.dtype)  # [seq_len, head_size//2]\n",
    "        sin = self.sin_matrix[:seq_len].to(x.dtype)  # [seq_len, head_size//2]\n",
    "\n",
    "        # –Ø–≤–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ñ–æ—Ä–º—ã –¥–ª—è broadcasting\n",
    "        cos = cos.reshape(1, 1, seq_len, head_size // 2)\n",
    "        sin = sin.reshape(1, 1, seq_len, head_size // 2)\n",
    "\n",
    "        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —á–µ—Ç–Ω—ã–µ –∏ –Ω–µ—á–µ—Ç–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ø–æ –ü–û–°–õ–ï–î–ù–ï–ú–£ –∏–∑–º–µ—Ä–µ–Ω–∏—é\n",
    "        x_even = x[..., 0::2]  # [batch_size, num_heads, seq_len, head_size//2]\n",
    "        x_odd = x[..., 1::2]   # [batch_size, num_heads, seq_len, head_size//2]\n",
    "\n",
    "        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ–≤–æ—Ä–æ—Ç: q' = q * cos(mŒ∏) + rotate(q) * sin(mŒ∏)\n",
    "        x_rotated_even = x_even * cos - x_odd * sin\n",
    "        x_rotated_odd = x_even * sin + x_odd * cos\n",
    "\n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ –∏—Å—Ö–æ–¥–Ω—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å\n",
    "        x_rotated = torch.stack([x_rotated_even, x_rotated_odd], dim=-1)\n",
    "        x_rotated = x_rotated.flatten(-2)  # [batch_size, seq_len, head_size]\n",
    "\n",
    "        return x_rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "811921b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiQueryAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_q_heads: int,\n",
    "        emb_size: int,\n",
    "        head_size: int,\n",
    "        max_seq_len: int,\n",
    "        rope: RoPE = None,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._num_q_heads = num_q_heads\n",
    "        self._head_size = head_size\n",
    "        self._max_seq_len = max_seq_len\n",
    "        self._rope = rope\n",
    "        \n",
    "        self._q = nn.Linear(emb_size, num_q_heads * head_size)\n",
    "        self._k = nn.Linear(emb_size,  head_size)\n",
    "        self._v = nn.Linear(emb_size,  head_size)\n",
    "\n",
    "        # –°–æ–∑–¥–∞–Ω–∏–µ causal –º–∞—Å–∫–∏\n",
    "        mask = torch.tril(torch.ones(max_seq_len, max_seq_len))\n",
    "        self.register_buffer(\n",
    "            \"_tril_mask\", mask.bool() if hasattr(torch, \"bool\") else mask.byte()\n",
    "        )\n",
    "        \n",
    "        self._layer = nn.Linear(num_q_heads * head_size, emb_size)\n",
    "        self._dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        mask: torch.Tensor = None,\n",
    "        use_cache: bool = True,\n",
    "        cache: list = None,\n",
    "    ):\n",
    "        batch_size, seq_len, emb_size = x.shape\n",
    "        if seq_len > self._max_seq_len:\n",
    "            raise ValueError(\n",
    "                f\"–î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ {seq_len} –ø—Ä–µ–≤—ã—à–∞–µ—Ç –º–∞–∫—Å–∏–º—É–º {self._max_seq_len}\"\n",
    "            )\n",
    "\n",
    "        # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç–µ —Ç–µ–Ω–∑–æ—Ä x —á–µ—Ä–µ–∑ –º–∞—Ç—Ä–∏—Ü—ã Wq, Wk , Wv, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –º–∞—Ç—Ä–∏—Ü—ã –∑–∞–ø—Ä–æ—Å–∞, –∫–ª—é—á–∞ –∏ –∑–Ω–∞—á–µ–Ω–∏—è.\n",
    "        k = self._k(x)  # [B, T, hs]\n",
    "        q = self._q(x)  # [B, T, hs]\n",
    "        v = self._v(x)  # [B, T, hs]\n",
    "\n",
    "        # –®–∞–≥ 2: –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ñ–æ—Ä–º—ã –¥–ª—è multi-head\n",
    "        # [batch_size, seq_len, num_heads * head_size] \n",
    "        # -> [batch_size, seq_len, num_heads, head_size]\n",
    "        q = q.reshape(batch_size, seq_len, self._num_q_heads, self._head_size)\n",
    "        k = k.reshape(batch_size, seq_len, 1, self._head_size)\n",
    "        v = v.reshape(batch_size, seq_len, 1, self._head_size)\n",
    "\n",
    "        # 3. Transpose: [B, T, H, hs] -> [B, H, T, hs]\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç–µ –º–∞—Ç—Ä–∏—Ü—ã –∑–∞–ø—Ä–æ—Å–∞ –∏ –∫–ª—é—á–∞ —á–µ—Ä–µ–∑ —ç–∫–∑–µ–º–ø–ª—è—Ä rope, —á—Ç–æ–±—ã –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–≤–æ—Ä–æ—Ç.\n",
    "        if self._rope is not None:\n",
    "            # –ü—Ä–∏–º–µ–Ω—è–µ–º RoPE –∫ Q –∏ K (–ù–ï –∫ V!)\n",
    "            q = self._rope(q)  # [B, T, hs]\n",
    "            k = self._rope(k)  # [B, T, hs]\n",
    "\n",
    "\n",
    "        # –ï—Å–ª–∏ cache –ø—Ä–∏—à–µ–ª, —Ç–æ –æ–±—ä–µ–¥–∏–Ω—è–µ–º –∫—ç—à –∏ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –∏–∑ –∫–ª—é—á–∞ –∏ –∑–Ω–∞—á–µ–Ω–∏—è. –≠—Ç–æ –±—É–¥—É—Ç –Ω–æ–≤—ã–µ key –∏ value  –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.\n",
    "        # 5. –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ (–¥–ª—è autoregressive generation)\n",
    "        if cache is not None:\n",
    "            k_cache, v_cache = cache\n",
    "            k = torch.cat([k_cache, k], dim=2)  # Concat –ø–æ seq_len (dim=2)\n",
    "            v = torch.cat([v_cache, v], dim=2)\n",
    "\n",
    "\n",
    "        # –ü–µ—Ä–µ–º–Ω–æ–∂–∏–º –º–∞—Ç—Ä–∏—Ü—ã –∑–∞–ø—Ä–æ—Å–∞ –∏ –∫–ª—é—á–∞ (—Ç—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω—É—é), —á—Ç–æ–±—ã –≤—ã—á–∏—Å–ª–∏—Ç—å –º–∞—Ç—Ä–∏—Ü—É –≤–Ω–∏–º–∞–Ω–∏—è.\n",
    "        # –ò —Ä–∞–∑–¥–µ–ª–∏—Ç—å –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –º–∞—Ç—Ä–∏—Ü–µ –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ –∫–æ—Ä–µ–Ω—å –∏–∑ head_size.\n",
    "        scores = q @ k.transpose(-2, -1) / (self._head_size ** 0.5)\n",
    "\n",
    "        # –ï—Å–ª–∏ cache –ø—Ä–∏—à–µ–ª, —Ç–æ –º–∞—Å–∫—É –Ω–µ –Ω–∞–∫–ª–∞–¥—ã–≤–∞–µ–º. –ò–Ω–∞—á–µ –Ω–∞–ª–æ–∂–∏—Ç–µ –Ω–∞ –º–∞—Ç—Ä–∏—Ü—É –≤–Ω–∏–º–∞–Ω–∏—è —Ç—Ä–µ—É–≥–æ–ª—å–Ω—É—é –º–∞—Å–∫—É, —Å–æ–∑–¥–∞–Ω–Ω—É—é –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏. –í—Å–µ —Å–∫—Ä—ã—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø—Ä–∏–≤–µ–¥–µ–Ω—ã –∫ –º–∏–Ω—É—Å –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ—Å—Ç–∏: float('-inf').\n",
    "        if cache is None:\n",
    "            scores = scores.masked_fill(\n",
    "                ~self._tril_mask[:seq_len, :seq_len], float(\"-inf\")\n",
    "            )\n",
    "\n",
    "        # –ü—Ä–∏–º–µ–Ω–∏—Ç—å –∫ –º–∞—Ç—Ä–∏—Ü–µ –≤–Ω–∏–º–∞–Ω–∏—è (–ø–æ—Å—Ç—Ä–æ—á–Ω–æ) —Ñ—É–Ω–∫—Ü–∏—é Softmax.\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # –ü–µ—Ä–µ–º–Ω–æ–∂–∏–º –º–∞—Ç—Ä–∏—Ü—É –≤–Ω–∏–º–∞–Ω–∏—è –∏ –º–∞—Ç—Ä–∏—Ü—É –∑–Ω–∞—á–µ–Ω–∏—è.\n",
    "        x_out = weights @ v  # [B, T, hs]\n",
    "\n",
    "\n",
    "        # –ò–∑–º–µ–Ω–∏—Ç–µ —Ñ–æ—Ä–º—É —Ç–µ–Ω–∑–æ—Ä–∞ –Ω–∞ batch_size √ó seq_len √ó num_heads*head_size.\n",
    "        # Transpose –æ–±—Ä–∞—Ç–Ω–æ –∏ concatenate heads\n",
    "        x_out = x_out.transpose(1, 2)  # [B, T_q, H, hs]\n",
    "        x_out = x_out.contiguous()  # –í–∞–∂–Ω–æ –¥–ª—è reshape!\n",
    "        concatenated_attention = x_out.reshape(batch_size, seq_len, self._num_q_heads * self._head_size)\n",
    "\n",
    "\n",
    "        # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç–µ –ø–æ–ª—É—á–∏–≤—à–∏–π—Å—è —Ç–µ–Ω–∑–æ—Ä —á–µ—Ä–µ–∑ –ø–æ—Å–ª–µ–¥–Ω–∏–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π.\n",
    "        # 3. –ü—Ä–æ–µ—Ü–∏—Ä—É–µ–º –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "        projected_output = self._layer(concatenated_attention)\n",
    "\n",
    "\n",
    "        # 4. –ü—Ä–∏–º–µ–Ω—è–µ–º dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏\n",
    "        final_output = self._dropout(projected_output)\n",
    "\n",
    "        if use_cache is True:\n",
    "            return (final_output, (k, v))\n",
    "        else:\n",
    "            return (final_output, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97771d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test 1 - Output shape: torch.Size([2, 10, 512])\n",
      "‚úÖ Test 2 - First output shape: torch.Size([2, 5, 512])\n",
      "‚úÖ Test 2 - Second output shape: torch.Size([2, 1, 512])\n",
      "\n",
      "‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "emb_size = 512\n",
    "num_q_heads = 8\n",
    "head_size = 64\n",
    "max_seq_len = 512\n",
    "\n",
    " # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "rope = RoPE(head_size=head_size, max_seq_len=max_seq_len)\n",
    "mha = MultiQueryAttention(\n",
    "    num_q_heads=num_q_heads,\n",
    "    emb_size=emb_size,\n",
    "    head_size=head_size,\n",
    "    max_seq_len=max_seq_len,\n",
    "    rope=rope,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    " # –¢–µ—Å—Ç 1: –û–±—ã—á–Ω—ã–π forward pass\n",
    "x = torch.randn(batch_size, seq_len, emb_size)\n",
    "output, cache = mha(x, use_cache=False)\n",
    "print(f\"‚úÖ Test 1 - Output shape: {output.shape}\")  # [2, 10, 512]\n",
    "assert output.shape == (batch_size, seq_len, emb_size)\n",
    "\n",
    " # –¢–µ—Å—Ç 2: –° –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º\n",
    "x1 = torch.randn(batch_size, 5, emb_size)\n",
    "output1, cache1 = mha(x1, use_cache=True)\n",
    "print(f\"‚úÖ Test 2 - First output shape: {output1.shape}\")  # [2, 5, 512]\n",
    "\n",
    "x2 = torch.randn(batch_size, 1, emb_size)\n",
    "output2, cache2 = mha(x2, use_cache=True, cache=cache1)\n",
    "print(f\"‚úÖ Test 2 - Second output shape: {output2.shape}\")  # [2, 1, 512]\n",
    "\n",
    "print(\"\\n‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5875022",
   "metadata": {},
   "source": [
    "–í–æ—Ç –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Markdown –¥–ª—è —Ç–≤–æ–µ–≥–æ HTML:\n",
    "\n",
    "---\n",
    "\n",
    "# GeGLU\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://ucarecdn.com/292c5458-2544-4e1f-a3f1-a981faeab39d/\" alt=\"geglu\" width=\"1000\" height=\"586\">\n",
    "</p>\n",
    "\n",
    "GeGLU ‚Äî —ç—Ç–æ –≥–∏–±—Ä–∏–¥–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏.\n",
    "–ü–æ —Å—É—Ç–∏, —ç—Ç–æ —Ç–∞ –∂–µ **SwiGLU**, –∫–æ—Ç–æ—Ä—É—é –º—ã —Ä–µ–∞–ª–∏–∑–æ–≤–∞–ª–∏ –≤ **Llama**, –ø—Ä–æ—Å—Ç–æ —É –Ω–µ—ë –≤ –∫–∞—á–µ—Å—Ç–≤–µ –±–∞–∑–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –≤–º–µ—Å—Ç–æ **SiLU** (–∫–∞–∫ –≤ Llama) –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è **GELU** (–∫–∞–∫ –≤ GPT-2).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3345832",
   "metadata": {},
   "source": [
    "**GeGLU (—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "82f52110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sqrt_2_over_pi = torch.sqrt(torch.tensor(2.0) / math.pi)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            self.sqrt_2_over_pi * (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "class GeGLU(nn.Module):\n",
    "    def __init__(self, emb_size: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self._gate = nn.Linear(emb_size, 4 * emb_size)\n",
    "        self._up = nn.Linear(emb_size, 4 * emb_size)\n",
    "        self._down = nn.Linear(4 * emb_size, emb_size)\n",
    "        self._activation = GELU()\n",
    "        self._dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor): # [batch_size √ó seq_len √ó emb_size].\n",
    "        gate_out = self._gate(x)                          # [batch, seq, 4*emb]\n",
    "        activation_out = self._activation(gate_out)       # [batch, seq, 4*emb]\n",
    "        up_out = self._up(x)                              # [batch, seq, 4*emb]\n",
    "        out = up_out * activation_out                     # –ø–æ—ç–ª–µ–º–µ–Ω—Ç–Ω–æ–µ!\n",
    "        out = self._down(out)                             # [batch, seq, emb]\n",
    "        return self._dropout(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db378855",
   "metadata": {},
   "source": [
    "# Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "568437e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "    \n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self._eps = eps\n",
    "        self._w = nn.Parameter(torch.ones(dim))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor): # [batch_size √ó seq_len √ó emb_size]\n",
    "        rms = (x.pow(2).mean(-1, keepdim=True) + self._eps) ** 0.5\n",
    "        norm_x = x / rms\n",
    "        return self._w * norm_x\n",
    "\n",
    "class TokenEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size: int):\n",
    "        super().__init__()\n",
    "        self._embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=emb_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._embedding(x)\n",
    "\n",
    "    @property\n",
    "    def num_embeddings(self) -> int:\n",
    "        return self._embedding.num_embeddings\n",
    "\n",
    "    @property\n",
    "    def embedding_dim(self) -> int:\n",
    "        return self._embedding.embedding_dim\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class RoPE(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size: int, max_seq_len: int, base: int = 10_000):\n",
    "        super().__init__()\n",
    "        assert head_size % 2 == 0, \"head_size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —á–µ—Ç–Ω—ã–º\"\n",
    "\n",
    "        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç: Œ∏_i = base^(-2i/d) –¥–ª—è i ‚àà [0, d/2-1]\n",
    "        freqs = 1.0 / (base ** (2 * torch.arange(head_size // 2).float() / head_size))\n",
    "\n",
    "        # –ü–æ–∑–∏—Ü–∏–∏ –æ—Ç 0 –¥–æ max_seq_len-1\n",
    "        positions = torch.arange(max_seq_len).float()\n",
    "\n",
    "        # –í–Ω–µ—à–Ω–µ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ: m * Œ∏_i –¥–ª—è –≤—Å–µ—Ö –ø–æ–∑–∏—Ü–∏–π –∏ —á–∞—Å—Ç–æ—Ç\n",
    "        freq_matrix = positions.unsqueeze(1) * freqs.unsqueeze(0)\n",
    "\n",
    "        # –ü—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü –∫–æ—Å–∏–Ω—É—Å–æ–≤ –∏ —Å–∏–Ω—É—Å–æ–≤\n",
    "        self.register_buffer(\"cos_matrix\", torch.cos(freq_matrix))\n",
    "        self.register_buffer(\"sin_matrix\", torch.sin(freq_matrix))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: # [batch_size √ó seq_len √ó head_size] [batch_size √ó num_heads √ó seq_len √ó head_size]\n",
    "        batch_size, num_heads, seq_len, head_size = x.shape\n",
    "\n",
    "        # –ë–µ—Ä–µ–º –Ω—É–∂–Ω—É—é —á–∞—Å—Ç—å –º–∞—Ç—Ä–∏—Ü –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ —Ç–∏–ø—É x\n",
    "        cos = self.cos_matrix[:seq_len].to(x.dtype)  # [seq_len, head_size//2]\n",
    "        sin = self.sin_matrix[:seq_len].to(x.dtype)  # [seq_len, head_size//2]\n",
    "\n",
    "        # –Ø–≤–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ñ–æ—Ä–º—ã –¥–ª—è broadcasting\n",
    "        cos = cos.reshape(1, 1, seq_len, head_size // 2)\n",
    "        sin = sin.reshape(1, 1, seq_len, head_size // 2)\n",
    "\n",
    "        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —á–µ—Ç–Ω—ã–µ –∏ –Ω–µ—á–µ—Ç–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ø–æ –ü–û–°–õ–ï–î–ù–ï–ú–£ –∏–∑–º–µ—Ä–µ–Ω–∏—é\n",
    "        x_even = x[..., 0::2]  # [batch_size, num_heads, seq_len, head_size//2]\n",
    "        x_odd = x[..., 1::2]   # [batch_size, num_heads, seq_len, head_size//2]\n",
    "\n",
    "        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ–≤–æ—Ä–æ—Ç: q' = q * cos(mŒ∏) + rotate(q) * sin(mŒ∏)\n",
    "        x_rotated_even = x_even * cos - x_odd * sin\n",
    "        x_rotated_odd = x_even * sin + x_odd * cos\n",
    "\n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ –∏—Å—Ö–æ–¥–Ω—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å\n",
    "        x_rotated = torch.stack([x_rotated_even, x_rotated_odd], dim=-1)\n",
    "        x_rotated = x_rotated.flatten(-2)  # [batch_size, seq_len, head_size]\n",
    "\n",
    "        return x_rotated\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "        num_q_heads: int,\n",
    "        emb_size: int,\n",
    "        head_size: int,\n",
    "        max_seq_len: int,\n",
    "        rope: RoPE,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._heads = MultiQueryAttention(\n",
    "            num_q_heads=num_q_heads, \n",
    "            emb_size=emb_size, \n",
    "            head_size=head_size, \n",
    "            max_seq_len=max_seq_len,\n",
    "            rope=rope,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self._ff = GeGLU(emb_size=emb_size, dropout=dropout)\n",
    "        self._norm1 = RMSNorm(emb_size)\n",
    "        self._norm2 = RMSNorm(emb_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None, use_cache: bool = True, cache: list = None) -> torch.Tensor:\n",
    "        norm1_out = self._norm1(x)\n",
    "        attention, kv_caches = self._heads(norm1_out, mask, use_cache=use_cache, cache=cache)\n",
    "        out = attention + x\n",
    "        \n",
    "        norm2_out = self._norm2(out)\n",
    "        ffn_out = self._ff(norm2_out)\n",
    "\n",
    "        if use_cache is True:\n",
    "            return (ffn_out + out, kv_caches)\n",
    "        else:\n",
    "            return (ffn_out + out, None)\n",
    "\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Gemma(nn.Module):\n",
    "    def __init__(self,\n",
    "        vocab_size: int,\n",
    "        max_seq_len: int,\n",
    "        emb_size: int,\n",
    "        num_q_heads: int,\n",
    "        head_size: int,\n",
    "        num_layers: int,\n",
    "        dropout: float = 0.1,\n",
    "        device: str = 'cpu'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._vocab_size = vocab_size\n",
    "        self._max_seq_len = max_seq_len\n",
    "        self._emb_size = emb_size\n",
    "        self._num_q_heads = num_q_heads\n",
    "        self._head_size = head_size\n",
    "        self._num_layers = num_layers\n",
    "        self._dropout = dropout\n",
    "        self._device = device\n",
    "        \n",
    "        self.validation_loss = None\n",
    "\n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ–µ–≤\n",
    "        self._token_embeddings = TokenEmbeddings(\n",
    "            vocab_size=vocab_size, \n",
    "            emb_size=emb_size\n",
    "        )\n",
    "        self._position_embeddings = RoPE(\n",
    "            head_size=head_size,\n",
    "            max_seq_len=max_seq_len\n",
    "        )\n",
    "        #self._position_embeddings = PositionalEmbeddings(\n",
    "        #    max_seq_len=max_seq_len, \n",
    "        #    emb_size=emb_size\n",
    "        #)\n",
    "        self._dropout = nn.Dropout(dropout)\n",
    "        self._decoders = nn.ModuleList([Decoder(\n",
    "            num_q_heads=num_q_heads,\n",
    "            emb_size=emb_size,\n",
    "            head_size=head_size,\n",
    "            max_seq_len=max_seq_len,\n",
    "            rope=self._position_embeddings,\n",
    "            dropout=dropout \n",
    "        ) for _ in range(num_layers)])\n",
    "        self._norm = RMSNorm(emb_size)\n",
    "        self._linear = nn.Linear(emb_size, vocab_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, use_cache: bool = True, cache: list = None) -> tuple:\n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –∫—ç—à–∞)\n",
    "        if cache is None and x.size(1) > self._max_seq_len:\n",
    "            raise ValueError(f\"–î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ {x.size(1)} –ø—Ä–µ–≤—ã—à–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é {self.max_seq_len}\")\n",
    "        \n",
    "        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–æ–∫–µ–Ω–æ–≤ –∏ –ø–æ–∑–∏—Ü–∏–π\n",
    "        tok_out = self._token_embeddings(x)  # [batch, seq_len, emb_size]\n",
    "       #pos_out = self._position_embeddings(x)  # [batch, seq_len, emb_size]\n",
    "        \n",
    "        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "        out = self._dropout(tok_out)  # [batch, seq_len, emb_size]\n",
    "        \n",
    "        # –°—Ç–µ–∫ –¥–µ–∫–æ–¥–µ—Ä–æ–≤ —Å –ø–µ—Ä–µ–¥–∞—á–µ–π –∫—ç—à–∞\n",
    "        new_cache = []\n",
    "        for i, decoder in enumerate(self._decoders):\n",
    "            decoder_cache = cache[i] if cache is not None else None\n",
    "            decoder_result = decoder(out, use_cache=use_cache, cache=decoder_cache)\n",
    "\n",
    "            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ –∫–æ—Ä—Ç–µ–∂–∞\n",
    "            if use_cache:\n",
    "                out, decoder_new_cache = decoder_result\n",
    "                new_cache.append(decoder_new_cache)\n",
    "            else:\n",
    "                out = decoder_result[0]\n",
    "\n",
    "        out = self._norm(out)\n",
    "        logits = self._linear(out)\n",
    "            \n",
    "        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å —É—á–µ—Ç–æ–º use_cache\n",
    "        if use_cache:\n",
    "            return (logits, new_cache)\n",
    "        else:\n",
    "            return (logits, None)\n",
    "\n",
    "    def generate(self,\n",
    "        x: torch.Tensor, \n",
    "        max_new_tokens: int, \n",
    "        do_sample: bool,\n",
    "        temperature: float = 1.0,\n",
    "        top_k: int = None,\n",
    "        top_p: float = None,\n",
    "        use_cache: bool = True\n",
    "    ) -> torch.Tensor:\n",
    "        cache = None\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            if use_cache and cache is not None:\n",
    "                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à - –ø–µ—Ä–µ–¥–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω\n",
    "                x_input = x[:, -1:]  # [batch_size, 1]\n",
    "            else:\n",
    "                # –ü–µ—Ä–≤–∞—è –∏—Ç–µ—Ä–∞—Ü–∏—è –∏–ª–∏ –∫—ç—à –æ—Ç–∫–ª—é—á–µ–Ω - –ø–µ—Ä–µ–¥–∞–µ–º –≤—Å—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å\n",
    "                x_input = x\n",
    "            \n",
    "            # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —Å –∫—ç—à–µ–º\n",
    "            logits, new_cache = self.forward(x_input, use_cache=use_cache, cache=cache)\n",
    "            \n",
    "            # –û–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏\n",
    "            if use_cache:\n",
    "                cache = new_cache\n",
    "\n",
    "            last_logits = logits[:, -1, :]  # [batch_size, vocab_size]\n",
    "\n",
    "            # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –ª–æ–≥–∏—Ç—ã —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ–π\n",
    "            if temperature > 0:\n",
    "                logits_scaled = last_logits / temperature\n",
    "            else:\n",
    "                logits_scaled = last_logits\n",
    "\n",
    "            if do_sample == True and top_k != None:\n",
    "                _, topk_indices = torch.topk(logits_scaled, top_k, dim=-1)\n",
    "\n",
    "                # # –ó–∞–º–µ–Ω–∏–º –≤—Å–µ –ù–ï top-k –ª–æ–≥–∏—Ç—ã –Ω–∞ -inf\n",
    "                masked_logits = logits_scaled.clone()\n",
    "                vocab_size = logits_scaled.size(-1)\n",
    "\n",
    "                # —Å–æ–∑–¥–∞—ë–º –º–∞—Å–∫—É: 1, –µ—Å–ª–∏ —Ç–æ–∫–µ–Ω –ù–ï –≤ topk_indices\n",
    "                mask = torch.ones_like(logits_scaled, dtype=torch.uint8)\n",
    "                mask.scatter_(1, topk_indices, 0)  # 0 —Ç–∞–º, –≥–¥–µ top-k –∏–Ω–¥–µ–∫—Å—ã\n",
    "                masked_logits[mask.byte()] = float('-inf')\n",
    "\n",
    "                logits_scaled = masked_logits\n",
    "\n",
    "            if do_sample == True and top_p != None:\n",
    "                # 1. –ü—Ä–∏–º–µ–Ω–∏–º softmax, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:\n",
    "                probs = F.softmax(logits_scaled, dim=-1)  # [B, vocab_size]\n",
    "                # 2. –û—Ç—Å–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω—ã –ø–æ —É–±—ã–≤–∞–Ω–∏—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π:\n",
    "                sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)\n",
    "                # 3. –ü–æ—Å—á–∏—Ç–∞–µ–º –∫—É–º—É–ª—è—Ç–∏–≤–Ω—É—é —Å—É–º–º—É –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π:\n",
    "                cum_probs = torch.cumsum(sorted_probs, dim=-1)  # [B, vocab_size]\n",
    "                # 4. –û–ø—Ä–µ–¥–µ–ª–∏–º –º–∞—Å–∫—É: –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–∫–µ–Ω—ã, –ø–æ–∫–∞ —Å—É–º–º–∞ < top_p\n",
    "                sorted_mask = (cum_probs <= top_p).byte()  # [B, vocab_size]\n",
    "                # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ —Ö–æ—Ç—è –±—ã –ø–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω –æ—Å—Ç–∞–Ω–µ—Ç—Å—è\n",
    "                sorted_mask[:, 0] = 1\n",
    "                # 5. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–∞—Å–∫—É –æ–±—Ä–∞—Ç–Ω–æ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫:\n",
    "                # –°–æ–∑–¥–∞—ë–º –ø–æ–ª–Ω—É—é –º–∞—Å–∫—É –∏–∑ 0\n",
    "                mask = torch.zeros_like(probs, dtype=torch.uint8)\n",
    "                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º 1 –≤ –º–µ—Å—Ç–∞—Ö –Ω—É–∂–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
    "                mask.scatter_(dim=1, index=sorted_indices, src=sorted_mask)\n",
    "                # 6. –ó–∞–Ω—É–ª—è–µ–º –ª–æ–≥–∏—Ç—ã —Ç–æ–∫–µ–Ω–æ–≤ –≤–Ω–µ —Ç–æ–ø-p:\n",
    "                logits_scaled[~mask] = float('-inf')\n",
    "\n",
    "            # 4. –ü—Ä–∏–º–µ–Ω—è–µ–º Softmax\n",
    "            probs = F.softmax(logits_scaled, dim=-1)  # [batch_size, vocab_size]\n",
    "\n",
    "\n",
    "            if do_sample == True:\n",
    "                # 5. –ï—Å–ª–∏ do_sample —Ä–∞–≤–µ–Ω True, —Ç–æ –æ—Ç–±–∏—Ä–∞–µ–º —Ç–æ–∫–µ–Ω —Å–ª—É—á–∞–π–Ω–æ —Å –ø–æ–º–æ—â—å—é torch.multinomial\n",
    "                next_token = torch.multinomial(probs, num_samples=1)  # [batch_size, 1]\n",
    "            else:\n",
    "                # 5. –ï—Å–ª–∏ do_sample —Ä–∞–≤–µ–Ω False, —Ç–æ –≤—ã–±–∏—Ä–∞–µ–º —Ç–æ–∫–µ–Ω —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é\n",
    "                next_token = torch.argmax(probs, dim=-1, keepdim=True)  # [batch_size, 1]\n",
    "            \n",
    "            # 6. –î–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "            x = torch.cat([x, next_token], dim=1)  # [batch_size, seq_len+1]\n",
    "        return x\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save({\n",
    "            'model_state_dict': self.state_dict(),\n",
    "            'vocab_size': self._vocab_size,\n",
    "            'max_seq_len': self._max_seq_len,\n",
    "            'emb_size': self._emb_size,\n",
    "            'num_heads': self._num_heads,\n",
    "            'head_size': self._head_size,\n",
    "            'num_layers': self._num_layers\n",
    "        }, path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path, device):\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        model = cls(\n",
    "            vocab_size=checkpoint['vocab_size'],\n",
    "            max_seq_len=checkpoint['max_seq_len'],\n",
    "            emb_size=checkpoint['emb_size'],\n",
    "            num_heads=checkpoint['num_heads'],\n",
    "            head_size=checkpoint['head_size'],\n",
    "            num_layers=checkpoint['num_layers']\n",
    "        )\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(device)\n",
    "        return model\n",
    "\n",
    "    @property\n",
    "    def max_seq_len(self) -> int:\n",
    "        return self._max_seq_len\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42746fea",
   "metadata": {},
   "source": [
    "## 2. –û–±—É—á–µ–Ω–∏–µ Gemma\n",
    "\n",
    "Gemma –æ–±—É—á–∞–µ—Ç—Å—è –≤ –¥–≤–∞ —ç—Ç–∞–ø–∞:\n",
    "\n",
    "- 1Ô∏è‚É£ **–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ (Unsupervised Pretraining)**  \n",
    "- 2Ô∏è‚É£ **–î–æ–æ–±—É—á–µ–Ω–∏–µ (Supervised Fine-Tuning)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b0234d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 5.1 –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ\n",
    "\n",
    "–ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –±–µ–∑ —Ä–∞–∑–º–µ—Ç–∫–∏: –æ–Ω–∞ –ø–æ–ª—É—á–∞–µ—Ç –±–æ–ª—å—à–æ–π –∫–æ—Ä–ø—É—Å —Ç–µ–∫—Å—Ç–æ–≤ –∏ —É—á–∏—Ç—Å—è **–ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω** –ø–æ –ø—Ä–µ–¥—ã–¥—É—â–∏–º.\n",
    "\n",
    "–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å:\n",
    "$$\n",
    "L = - \\sum_{t=1}^{T} \\log P(x_t | x_1, x_2, ..., x_{t-1})\n",
    "$$\n",
    "\n",
    "–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è —Å—Ç—Ä–æ–∏—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—É—é –º–æ–¥–µ–ª—å —è–∑—ã–∫–∞, \"—É–≥–∞–¥—ã–≤–∞—è\" –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c94641",
   "metadata": {},
   "source": [
    "–í–æ –≤—Ä–µ–º—è **–ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è** Mistral —É—á–∏—Ç—Å—è **–ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω** (language modeling task).  \n",
    "–§–æ—Ä–º–∞–ª—å–Ω–æ:  \n",
    "$$ \n",
    "P(x_t ,|, x_1, x_2, \\dots, x_{t-1})  \n",
    "$$ \n",
    "–¢–æ –µ—Å—Ç—å, –µ—Å–ª–∏ –Ω–∞ –≤—Ö–æ–¥ –ø–æ–¥–∞—ë—Ç—Å—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ `\"I love deep\"`, –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å `\"learning\"`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b064fadc",
   "metadata": {},
   "source": [
    "### ‚úÖ 5.1.1 –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "–°–æ–∑–¥–∞–¥–∏–º **–¥–∞—Ç–∞—Å–µ—Ç** –Ω–∞ –æ—Å–Ω–æ–≤–µ BPE-—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1516a37",
   "metadata": {},
   "source": [
    "**BPE Tokenizator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8a5a975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPE:\n",
    "    def __init__(self, vocab_size: int):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.id2token = {}\n",
    "        self.token2id = {}\n",
    "\n",
    "    def fit(self, text: str):\n",
    "        # 1. –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã (—Å–∏–º–≤–æ–ª—ã)\n",
    "        unique_tokens = sorted(set(text))\n",
    "        tokens = unique_tokens.copy()\n",
    "\n",
    "        # 2. –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Ç–æ–∫–µ–Ω—ã-—Å–∏–º–≤–æ–ª—ã\n",
    "        sequence = list(text)\n",
    "\n",
    "        # 3. –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–æ–∫–µ–Ω—ã –¥–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –Ω—É–∂–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —Å–ª–æ–≤–∞—Ä—è\n",
    "        while len(tokens) < self.vocab_size:\n",
    "            #print(f'len={len(tokens)} < {self.vocab_size}')\n",
    "            # –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—ã –ø–∞—Ä\n",
    "            pair_freq = {}\n",
    "            for i in range(len(sequence) - 1):\n",
    "                pair = (sequence[i], sequence[i + 1])\n",
    "                #print(f'pair = {pair}')\n",
    "                if pair not in pair_freq:\n",
    "                    pair_freq[pair] = 0\n",
    "                pair_freq[pair] += 1\n",
    "\n",
    "\n",
    "            #print(f'pair_freq = {pair_freq}')  \n",
    "            if not pair_freq:\n",
    "                break  # –Ω–µ—Ç –ø–∞—Ä ‚Äî –≤—ã—Ö–æ–¥–∏–º\n",
    "\n",
    "            #for x in pair_freq.items():\n",
    "            #    self.debug(x, sequence)\n",
    "\n",
    "            # –ù–∞—Ö–æ–¥–∏–º —Å–∞–º—É—é —á–∞—Å—Ç—É—é –ø–∞—Ä—É (–≤ —Å–ª—É—á–∞–µ —Ä–∞–≤–µ–Ω—Å—Ç–≤–∞ ‚Äî —Ç–∞, —á—Ç–æ –≤—Å—Ç—Ä–µ—Ç–∏–ª–∞—Å—å –ø–µ—Ä–≤–æ–π)\n",
    "            most_frequent_pair = max(pair_freq.items(), key=lambda x: (x[1], -self._pair_first_index(sequence, x[0])))[0]\n",
    "            #print(most_frequent_pair)\n",
    "            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω\n",
    "            new_token = most_frequent_pair[0] + most_frequent_pair[1]\n",
    "            #print(f\"new token={new_token}\")\n",
    "            tokens.append(new_token)\n",
    "            #print(f\"tokens={tokens}\")\n",
    "\n",
    "            i = 0\n",
    "            new_sequence = []\n",
    "\n",
    "            while i < len(sequence):\n",
    "                if i < len(sequence) - 1 and (sequence[i], sequence[i + 1]) == most_frequent_pair:\n",
    "                    new_sequence.append(new_token)\n",
    "                    i += 2  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–≤–∞ —Å–∏–º–≤–æ–ª–∞ ‚Äî –∑–∞–º–µ–Ω—ë–Ω–Ω—É—é –ø–∞—Ä—É\n",
    "                else:\n",
    "                    new_sequence.append(sequence[i])\n",
    "                    i += 1\n",
    "            sequence = new_sequence\n",
    "            #break\n",
    "        \n",
    "        # 4. –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä–∏\n",
    "        self.vocab = tokens.copy()\n",
    "        self.token2id = dict(zip(tokens, range(self.vocab_size)))\n",
    "        self.id2token = dict(zip(range(self.vocab_size), tokens))\n",
    "\n",
    "    def _pair_first_index(self, sequence, pair):\n",
    "        for i in range(len(sequence) - 1):\n",
    "            if (sequence[i], sequence[i + 1]) == pair:\n",
    "                return i\n",
    "        return float('inf')  # –µ—Å–ª–∏ –ø–∞—Ä–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ (–≤ —Ç–µ–æ—Ä–∏–∏ –Ω–µ –¥–æ–ª–∂–Ω–æ —Å–ª—É—á–∏—Ç—å—Å—è)\n",
    "\n",
    "\n",
    "    def encode(self, text: str):\n",
    "        # 1. –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Ç–æ–∫–µ–Ω—ã-—Å–∏–º–≤–æ–ª—ã\n",
    "        sequence = list(text)\n",
    "        # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—É—Å—Ç–æ–≥–æ —Å–ø–∏—Å–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤\n",
    "        tokens = []\n",
    "        # 3. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å i = 0\n",
    "        i = 0\n",
    "        while i < len(text):\n",
    "            # 3.1 –ù–∞–π—Ç–∏ –≤—Å–µ —Ç–æ–∫–µ–Ω—ã –≤ —Å–ª–æ–≤–∞—Ä–µ, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å text[i]\n",
    "            start_char = text[i]\n",
    "            result = [token for token in self.vocab if token.startswith(start_char)]\n",
    "            # 3.2 –í—ã–±—Ä–∞—Ç—å —Å–∞–º—ã–π –¥–ª–∏–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥—è—â–∏–π —Ç–æ–∫–µ–Ω\n",
    "            find_token = self._find_max_matching_token(text[i:], result)\n",
    "            if find_token is None:\n",
    "                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞\n",
    "                tokens.append(text[i])  # –î–æ–±–∞–≤–ª—è–µ–º —Å–∞–º —Å–∏–º–≤–æ–ª –∫–∞–∫ —Ç–æ–∫–µ–Ω\n",
    "                i += 1\n",
    "            else:\n",
    "                # 3.3 –î–æ–±–∞–≤–∏—Ç—å —Ç–æ–∫–µ–Ω –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "                tokens.append(find_token)\n",
    "                # 3.4 –£–≤–µ–ª–∏—á–∏—Ç—å i –Ω–∞ –¥–ª–∏–Ω—É —Ç–æ–∫–µ–Ω–∞\n",
    "                i += len(find_token)\n",
    "\n",
    "        # 4. –ó–∞–º–µ–Ω–∏—Ç—å —Ç–æ–∫–µ–Ω—ã –Ω–∞ –∏—Ö ID\n",
    "        return self._tokens_to_ids(tokens)\n",
    "\n",
    "    def _find_max_matching_token(self, text: str, tokens: list):\n",
    "        \"\"\"–ù–∞—Ö–æ–¥–∏—Ç —Å–∞–º—ã–π –¥–ª–∏–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω –∏–∑ —Å–ø–∏—Å–∫–∞, —Å –∫–æ—Ç–æ—Ä–æ–≥–æ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Ç–µ–∫—Å—Ç\"\"\"\n",
    "        matching = [token for token in tokens if text.startswith(token)]\n",
    "        return max(matching, key=len) if matching else None\n",
    "\n",
    "    def _tokens_to_ids(self, tokens):\n",
    "        \"\"\"–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∏—Ö ID —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤\"\"\"\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            if token in self.token2id:\n",
    "                ids.append(self.token2id[token])\n",
    "            else:\n",
    "                ids.append(0)  # –°–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
    "        return ids\n",
    "\n",
    "\n",
    "    def decode(self, ids: list) -> str:\n",
    "        return ''.join(self._ids_to_tokens(ids))\n",
    "\n",
    "    def _ids_to_tokens(self, ids: list) -> list:\n",
    "        \"\"\"–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ Ids –≤ –∏—Ö tokens\"\"\"\n",
    "        tokens = []\n",
    "        for id in ids:\n",
    "            if id in self.id2token:\n",
    "                tokens.append(self.id2token[id])\n",
    "            else:\n",
    "                tokens.append('')  # –°–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
    "        return tokens\n",
    "\n",
    "\n",
    "    def save(self, filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            dill.dump(self, f)\n",
    "        print(f\"–û–±—ä–µ–∫—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {filename}\")\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            obj = dill.load(f)\n",
    "                \n",
    "        print(f\"–û–±—ä–µ–∫—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {filename}\")\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1927f6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, text: str, bpe: BPE, block_size: int):\n",
    "        self.bpe = bpe\n",
    "        self.block_size = block_size\n",
    "        self.data = bpe.encode(text)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx:idx+self.block_size], dtype=torch.long)\n",
    "        y = torch.tensor(self.data[idx+1:idx+self.block_size+1], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14d0c68",
   "metadata": {},
   "source": [
    "### ‚úÖ 5.1.2 –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "–î–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–æ–∑–¥–∞–¥–∏–º —Ñ—É–Ω–∫—Ü–∏—é:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7c5c57b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "def train_gemma(model, dataset, epochs=5, batch_size=32, lr=3e-4, device='cpu'):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥\n",
    "            logits, _ = model(x, use_cache=False)  # [B, T, vocab_size]\n",
    "\n",
    "            # –ü–µ—Ä–µ—Å—Ç—Ä–æ–∏–º –≤—ã—Ö–æ–¥ –ø–æ–¥ CrossEntropy\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "            # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96dea80",
   "metadata": {},
   "source": [
    "### ‚úÖ 5.1.3 –ü—Ä–∏–º–µ—Ä –∑–∞–ø—É—Å–∫–∞\n",
    "\n",
    "\n",
    "**üß† –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Gemma Mini**\n",
    "\n",
    "\n",
    "| –ü–∞—Ä–∞–º–µ—Ç—Ä        | –ó–Ω–∞—á–µ–Ω–∏–µ | –û–ø–∏—Å–∞–Ω–∏–µ                                      |\n",
    "| --------------- | -------- | --------------------------------------------- |\n",
    "| **vocab_size**  | `50257`  | –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (BPE —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä OpenAI)       |\n",
    "| **max_seq_len** | `512`   | –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ |\n",
    "| **emb_size**    | `256`    | –†–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ)   |\n",
    "| **num_heads**   | `4`     | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤ multi-head attention       |\n",
    "| **head_size**   | `64`     | –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –æ–¥–Ω–æ–π –≥–æ–ª–æ–≤—ã –≤–Ω–∏–º–∞–Ω–∏—è (768 / 12)  |\n",
    "| **num_layers**  | `4`     | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–æ–∫–æ–≤ (–¥–µ–∫–æ–¥–µ—Ä–æ–≤)                 |\n",
    "| **dropout**     | `0.1`    | –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥—Ä–æ–ø–∞—É—Ç–∞                          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cda62fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 20\n",
      "Epoch 1/100, Loss: 3.8178\n",
      "Epoch 2/100, Loss: 1.5683\n",
      "Epoch 3/100, Loss: 0.6454\n",
      "Epoch 4/100, Loss: 0.3353\n",
      "Epoch 5/100, Loss: 0.2306\n",
      "Epoch 6/100, Loss: 0.1581\n",
      "Epoch 7/100, Loss: 0.1253\n",
      "Epoch 8/100, Loss: 0.1063\n",
      "Epoch 9/100, Loss: 0.0923\n",
      "Epoch 10/100, Loss: 0.0909\n",
      "Epoch 11/100, Loss: 0.0761\n",
      "Epoch 12/100, Loss: 0.0932\n",
      "Epoch 13/100, Loss: 0.0775\n",
      "Epoch 14/100, Loss: 0.0797\n",
      "Epoch 15/100, Loss: 0.0623\n",
      "Epoch 16/100, Loss: 0.0795\n",
      "Epoch 17/100, Loss: 0.0703\n",
      "Epoch 18/100, Loss: 0.0581\n",
      "Epoch 19/100, Loss: 0.0613\n",
      "Epoch 20/100, Loss: 0.0660\n",
      "Epoch 21/100, Loss: 0.0731\n",
      "Epoch 22/100, Loss: 0.0644\n",
      "Epoch 23/100, Loss: 0.0602\n",
      "Epoch 24/100, Loss: 0.0557\n",
      "Epoch 25/100, Loss: 0.0595\n",
      "Epoch 26/100, Loss: 0.0688\n",
      "Epoch 27/100, Loss: 0.0545\n",
      "Epoch 28/100, Loss: 0.0561\n",
      "Epoch 29/100, Loss: 0.0581\n",
      "Epoch 30/100, Loss: 0.0627\n",
      "Epoch 31/100, Loss: 0.0555\n",
      "Epoch 32/100, Loss: 0.0538\n",
      "Epoch 33/100, Loss: 0.0531\n",
      "Epoch 34/100, Loss: 0.0535\n",
      "Epoch 35/100, Loss: 0.0474\n",
      "Epoch 36/100, Loss: 0.0516\n",
      "Epoch 37/100, Loss: 0.0540\n",
      "Epoch 38/100, Loss: 0.0533\n",
      "Epoch 39/100, Loss: 0.0519\n",
      "Epoch 40/100, Loss: 0.0606\n",
      "Epoch 41/100, Loss: 0.0489\n",
      "Epoch 42/100, Loss: 0.0513\n",
      "Epoch 43/100, Loss: 0.0563\n",
      "Epoch 44/100, Loss: 0.0522\n",
      "Epoch 45/100, Loss: 0.0512\n",
      "Epoch 46/100, Loss: 0.0490\n",
      "Epoch 47/100, Loss: 0.0469\n",
      "Epoch 48/100, Loss: 0.0500\n",
      "Epoch 49/100, Loss: 0.0497\n",
      "Epoch 50/100, Loss: 0.0532\n",
      "Epoch 51/100, Loss: 0.0557\n",
      "Epoch 52/100, Loss: 0.0480\n",
      "Epoch 53/100, Loss: 0.0593\n",
      "Epoch 54/100, Loss: 0.0498\n",
      "Epoch 55/100, Loss: 0.0476\n",
      "Epoch 56/100, Loss: 0.0496\n",
      "Epoch 57/100, Loss: 0.0445\n",
      "Epoch 58/100, Loss: 0.0494\n",
      "Epoch 59/100, Loss: 0.0572\n",
      "Epoch 60/100, Loss: 0.0490\n",
      "Epoch 61/100, Loss: 0.0580\n",
      "Epoch 62/100, Loss: 0.0499\n",
      "Epoch 63/100, Loss: 0.0501\n",
      "Epoch 64/100, Loss: 0.0538\n",
      "Epoch 65/100, Loss: 0.0484\n",
      "Epoch 66/100, Loss: 0.0520\n",
      "Epoch 67/100, Loss: 0.0527\n",
      "Epoch 68/100, Loss: 0.0501\n",
      "Epoch 69/100, Loss: 0.0506\n",
      "Epoch 70/100, Loss: 0.0480\n",
      "Epoch 71/100, Loss: 0.0470\n",
      "Epoch 72/100, Loss: 0.0498\n",
      "Epoch 73/100, Loss: 0.0484\n",
      "Epoch 74/100, Loss: 0.0435\n",
      "Epoch 75/100, Loss: 0.0456\n",
      "Epoch 76/100, Loss: 0.0480\n",
      "Epoch 77/100, Loss: 0.0477\n",
      "Epoch 78/100, Loss: 0.0494\n",
      "Epoch 79/100, Loss: 0.0490\n",
      "Epoch 80/100, Loss: 0.0474\n",
      "Epoch 81/100, Loss: 0.0462\n",
      "Epoch 82/100, Loss: 0.0432\n",
      "Epoch 83/100, Loss: 0.0447\n",
      "Epoch 84/100, Loss: 0.0482\n",
      "Epoch 85/100, Loss: 0.0493\n",
      "Epoch 86/100, Loss: 0.0452\n",
      "Epoch 87/100, Loss: 0.0417\n",
      "Epoch 88/100, Loss: 0.0489\n",
      "Epoch 89/100, Loss: 0.0487\n",
      "Epoch 90/100, Loss: 0.0486\n",
      "Epoch 91/100, Loss: 0.0451\n",
      "Epoch 92/100, Loss: 0.0443\n",
      "Epoch 93/100, Loss: 0.0442\n",
      "Epoch 94/100, Loss: 0.0486\n",
      "Epoch 95/100, Loss: 0.0464\n",
      "Epoch 96/100, Loss: 0.0429\n",
      "Epoch 97/100, Loss: 0.0461\n",
      "Epoch 98/100, Loss: 0.0496\n",
      "Epoch 99/100, Loss: 0.0476\n",
      "Epoch 100/100, Loss: 0.0441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gemma(\n",
       "  (_token_embeddings): TokenEmbeddings(\n",
       "    (_embedding): Embedding(100, 256)\n",
       "  )\n",
       "  (_position_embeddings): RoPE()\n",
       "  (_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (_decoders): ModuleList(\n",
       "    (0-3): 4 x Decoder(\n",
       "      (_heads): MultiQueryAttention(\n",
       "        (_rope): RoPE()\n",
       "        (_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (_k): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (_v): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (_layer): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (_ff): GeGLU(\n",
       "        (_gate): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (_up): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (_down): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        (_activation): GELU()\n",
       "        (_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (_norm1): RMSNorm()\n",
       "      (_norm2): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (_norm): RMSNorm()\n",
       "  (_linear): Linear(in_features=256, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
    "text = \"Deep learning is amazing. Transformers changed the world. Attention is all you need. GPT models revolutionized NLP.\"\n",
    "\n",
    "# 2. –û–±—É—á–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "bpe = BPE(vocab_size=100)\n",
    "bpe.fit(text)\n",
    "\n",
    "# 3. –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç\n",
    "dataset = GPTDataset(text, bpe, block_size=8)\n",
    "print(f\"Dataset length: {len(dataset)}\")\n",
    "\n",
    "# 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å\n",
    "model = Gemma(\n",
    "    vocab_size=len(bpe.vocab),  # —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è BPE\n",
    "    max_seq_len=512,           # GPT-2 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ 512 —Ç–æ–∫–µ–Ω–∞\n",
    "    emb_size=256,               # —Ä–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "    num_q_heads=4,               # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "    head_size=64,               # —Ä–∞–∑–º–µ—Ä –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã (256 / 4)\n",
    "    num_layers=4,              # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–æ–∫–æ–≤ Transformer\n",
    "    dropout=0.1                 # —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π dropout GPT-2\n",
    ")\n",
    "\n",
    "# 5. –û–±—É—á–∞–µ–º\n",
    "train_gemma(model, dataset, epochs=100, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a37671",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 5.2 –î–æ–æ–±—É—á–µ–Ω–∏–µ\n",
    "\n",
    "–ü–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è Gemma —É–∂–µ –∑–Ω–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ –≥—Ä–∞–º–º–∞—Ç–∏–∫—É —è–∑—ã–∫–∞.  \n",
    "–ù–∞ –≤—Ç–æ—Ä–æ–º —ç—Ç–∞–ø–µ –æ–Ω–∞ –¥–æ–æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, QA) —Å –ø–æ–º–æ—â—å—é —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.\n",
    "\n",
    "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏ —ç—Ç–æ –ø–æ—á—Ç–∏ —Ç–æ –∂–µ –æ–±—É—á–µ–Ω–∏–µ, —Ç–æ–ª—å–∫–æ:\n",
    "\n",
    "- –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å —É–∂–µ –æ–±—É—á–µ–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏.\n",
    "- –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
    "- –ú–æ–∂–Ω–æ —É–º–µ–Ω—å—à–∏—Ç—å —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è.\n",
    "- –ò–Ω–æ–≥–¥–∞ –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞—é—Ç —á–∞—Å—Ç—å —Å–ª–æ—ë–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —ç–º–±–µ–¥–¥–∏–Ω–≥–∏).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d062af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_gemma(model, dataset, epochs=3, batch_size=16, lr=1e-5, device='cpu', freeze_embeddings=True):\n",
    "    if freeze_embeddings:\n",
    "        for param in model._token_embeddings.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model._position_embeddings.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits, _ = model(x, use_cache=False)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Fine-tune Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "064dd678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 1/10, Loss: 4.9095\n",
      "Fine-tune Epoch 2/10, Loss: 2.8684\n",
      "Fine-tune Epoch 3/10, Loss: 1.7589\n",
      "Fine-tune Epoch 4/10, Loss: 1.3044\n",
      "Fine-tune Epoch 5/10, Loss: 1.0614\n",
      "Fine-tune Epoch 6/10, Loss: 0.8326\n",
      "Fine-tune Epoch 7/10, Loss: 0.6908\n",
      "Fine-tune Epoch 8/10, Loss: 0.5926\n",
      "Fine-tune Epoch 9/10, Loss: 0.5082\n",
      "Fine-tune Epoch 10/10, Loss: 0.4758\n"
     ]
    }
   ],
   "source": [
    "# –ù–∞–ø—Ä–∏–º–µ—Ä, –º—ã —Ö–æ—Ç–∏–º –¥–æ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ —Å—Ç–∏–ª–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ñ—Ä–∞–∑\n",
    "fine_tune_text = \"\"\"\n",
    "Transformers revolutionize NLP.\n",
    "Deep learning enables self-attention.\n",
    "GPT generates text autoregressively.\n",
    "\"\"\"\n",
    "\n",
    "dataset = GPTDataset(fine_tune_text, bpe, block_size=8)\n",
    "\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫ –¥–æ–æ–±—É—á–µ–Ω–∏—è\n",
    "fine_tune_gemma(model, dataset, epochs=10, batch_size=4, lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a496ddae",
   "metadata": {},
   "source": [
    "## üìù 6. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "645f777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, bpe, prompt: str, max_new_tokens=20, device='cpu'):\n",
    "    model.eval()\n",
    "    ids = torch.tensor([bpe.encode(prompt)], dtype=torch.long).to(device)\n",
    "    out = model.generate(ids, max_new_tokens=max_new_tokens, do_sample=True)\n",
    "    text = bpe.decode(out[0].tolist())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "14778ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep learningenena lf lenenssssf \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, bpe, \"Deep learning\", max_new_tokens=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b70d909",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
