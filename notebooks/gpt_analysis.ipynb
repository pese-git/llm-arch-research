{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6842e799",
   "metadata": {},
   "source": [
    "\n",
    "–ú–æ–¥–µ–ª—å **GPT-1 (Generative Pretrained Transformer)** ‚Äî —ç—Ç–æ –ø–µ—Ä–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏–¥–µ–∏ —Å–æ–∑–¥–∞–Ω–∏—è —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã **Transformer Decoder**, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω–æ–π –≤ —Ä–∞–±–æ—Ç–µ *‚ÄúImproving Language Understanding by Generative Pre-Training‚Äù* (OpenAI, 2018).\n",
    "–û–Ω–∞ –∑–∞–ª–æ–∂–∏–ª–∞ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç –≤—Å–µ—Ö –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö –ø–æ–∫–æ–ª–µ–Ω–∏–π GPT-–º–æ–¥–µ–ª–µ–π, –ø–æ–∫–∞–∑–∞–≤, —á—Ç–æ –º–æ–¥–µ–ª—å, –æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ –±–æ–ª—å—à–æ–º –∫–æ—Ä–ø—É—Å–µ —Ç–µ–∫—Å—Ç–æ–≤ –≤ —Ä–µ–∂–∏–º–µ **–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞**, —Å–ø–æ—Å–æ–±–Ω–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º –∑–∞–¥–∞—á–∞–º –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4fba924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed35205",
   "metadata": {},
   "source": [
    "## BPE Tokenizator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a6f2914",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPE:\n",
    "    def __init__(self, vocab_size: int):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.id2token = {}\n",
    "        self.token2id = {}\n",
    "\n",
    "    def fit(self, text: str):\n",
    "        # 1. –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã (—Å–∏–º–≤–æ–ª—ã)\n",
    "        unique_tokens = sorted(set(text))\n",
    "        tokens = unique_tokens.copy()\n",
    "\n",
    "        # 2. –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Ç–æ–∫–µ–Ω—ã-—Å–∏–º–≤–æ–ª—ã\n",
    "        sequence = list(text)\n",
    "\n",
    "        # 3. –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–æ–∫–µ–Ω—ã –¥–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –Ω—É–∂–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —Å–ª–æ–≤–∞—Ä—è\n",
    "        while len(tokens) < self.vocab_size:\n",
    "            #print(f'len={len(tokens)} < {self.vocab_size}')\n",
    "            # –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—ã –ø–∞—Ä\n",
    "            pair_freq = {}\n",
    "            for i in range(len(sequence) - 1):\n",
    "                pair = (sequence[i], sequence[i + 1])\n",
    "                #print(f'pair = {pair}')\n",
    "                if pair not in pair_freq:\n",
    "                    pair_freq[pair] = 0\n",
    "                pair_freq[pair] += 1\n",
    "\n",
    "\n",
    "            #print(f'pair_freq = {pair_freq}')  \n",
    "            if not pair_freq:\n",
    "                break  # –Ω–µ—Ç –ø–∞—Ä ‚Äî –≤—ã—Ö–æ–¥–∏–º\n",
    "\n",
    "            #for x in pair_freq.items():\n",
    "            #    self.debug(x, sequence)\n",
    "\n",
    "            # –ù–∞—Ö–æ–¥–∏–º —Å–∞–º—É—é —á–∞—Å—Ç—É—é –ø–∞—Ä—É (–≤ —Å–ª—É—á–∞–µ —Ä–∞–≤–µ–Ω—Å—Ç–≤–∞ ‚Äî —Ç–∞, —á—Ç–æ –≤—Å—Ç—Ä–µ—Ç–∏–ª–∞—Å—å –ø–µ—Ä–≤–æ–π)\n",
    "            most_frequent_pair = max(pair_freq.items(), key=lambda x: (x[1], -self._pair_first_index(sequence, x[0])))[0]\n",
    "            #print(most_frequent_pair)\n",
    "            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω\n",
    "            new_token = most_frequent_pair[0] + most_frequent_pair[1]\n",
    "            #print(f\"new token={new_token}\")\n",
    "            tokens.append(new_token)\n",
    "            #print(f\"tokens={tokens}\")\n",
    "\n",
    "            i = 0\n",
    "            new_sequence = []\n",
    "\n",
    "            while i < len(sequence):\n",
    "                if i < len(sequence) - 1 and (sequence[i], sequence[i + 1]) == most_frequent_pair:\n",
    "                    new_sequence.append(new_token)\n",
    "                    i += 2  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–≤–∞ —Å–∏–º–≤–æ–ª–∞ ‚Äî –∑–∞–º–µ–Ω—ë–Ω–Ω—É—é –ø–∞—Ä—É\n",
    "                else:\n",
    "                    new_sequence.append(sequence[i])\n",
    "                    i += 1\n",
    "            sequence = new_sequence\n",
    "            #break\n",
    "        \n",
    "        # 4. –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä–∏\n",
    "        self.vocab = tokens.copy()\n",
    "        self.token2id = dict(zip(tokens, range(self.vocab_size)))\n",
    "        self.id2token = dict(zip(range(self.vocab_size), tokens))\n",
    "\n",
    "    def _pair_first_index(self, sequence, pair):\n",
    "        for i in range(len(sequence) - 1):\n",
    "            if (sequence[i], sequence[i + 1]) == pair:\n",
    "                return i\n",
    "        return float('inf')  # –µ—Å–ª–∏ –ø–∞—Ä–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ (–≤ —Ç–µ–æ—Ä–∏–∏ –Ω–µ –¥–æ–ª–∂–Ω–æ —Å–ª—É—á–∏—Ç—å—Å—è)\n",
    "\n",
    "\n",
    "    def encode(self, text: str):\n",
    "        # 1. –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Ç–æ–∫–µ–Ω—ã-—Å–∏–º–≤–æ–ª—ã\n",
    "        sequence = list(text)\n",
    "        # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—É—Å—Ç–æ–≥–æ —Å–ø–∏—Å–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤\n",
    "        tokens = []\n",
    "        # 3. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å i = 0\n",
    "        i = 0\n",
    "        while i < len(text):\n",
    "            # 3.1 –ù–∞–π—Ç–∏ –≤—Å–µ —Ç–æ–∫–µ–Ω—ã –≤ —Å–ª–æ–≤–∞—Ä–µ, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å text[i]\n",
    "            start_char = text[i]\n",
    "            result = [token for token in self.vocab if token.startswith(start_char)]\n",
    "            # 3.2 –í—ã–±—Ä–∞—Ç—å —Å–∞–º—ã–π –¥–ª–∏–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥—è—â–∏–π —Ç–æ–∫–µ–Ω\n",
    "            find_token = self._find_max_matching_token(text[i:], result)\n",
    "            if find_token is None:\n",
    "                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞\n",
    "                tokens.append(text[i])  # –î–æ–±–∞–≤–ª—è–µ–º —Å–∞–º —Å–∏–º–≤–æ–ª –∫–∞–∫ —Ç–æ–∫–µ–Ω\n",
    "                i += 1\n",
    "            else:\n",
    "                # 3.3 –î–æ–±–∞–≤–∏—Ç—å —Ç–æ–∫–µ–Ω –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "                tokens.append(find_token)\n",
    "                # 3.4 –£–≤–µ–ª–∏—á–∏—Ç—å i –Ω–∞ –¥–ª–∏–Ω—É —Ç–æ–∫–µ–Ω–∞\n",
    "                i += len(find_token)\n",
    "\n",
    "        # 4. –ó–∞–º–µ–Ω–∏—Ç—å —Ç–æ–∫–µ–Ω—ã –Ω–∞ –∏—Ö ID\n",
    "        return self._tokens_to_ids(tokens)\n",
    "\n",
    "    def _find_max_matching_token(self, text: str, tokens: list):\n",
    "        \"\"\"–ù–∞—Ö–æ–¥–∏—Ç —Å–∞–º—ã–π –¥–ª–∏–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω –∏–∑ —Å–ø–∏—Å–∫–∞, —Å –∫–æ—Ç–æ—Ä–æ–≥–æ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Ç–µ–∫—Å—Ç\"\"\"\n",
    "        matching = [token for token in tokens if text.startswith(token)]\n",
    "        return max(matching, key=len) if matching else None\n",
    "\n",
    "    def _tokens_to_ids(self, tokens):\n",
    "        \"\"\"–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∏—Ö ID —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤\"\"\"\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            if token in self.token2id:\n",
    "                ids.append(self.token2id[token])\n",
    "            else:\n",
    "                ids.append(0)  # –°–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
    "        return ids\n",
    "\n",
    "\n",
    "    def decode(self, ids: list) -> str:\n",
    "        return ''.join(self._ids_to_tokens(ids))\n",
    "\n",
    "    def _ids_to_tokens(self, ids: list) -> list:\n",
    "        \"\"\"–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ Ids –≤ –∏—Ö tokens\"\"\"\n",
    "        tokens = []\n",
    "        for id in ids:\n",
    "            if id in self.id2token:\n",
    "                tokens.append(self.id2token[id])\n",
    "            else:\n",
    "                tokens.append('')  # –°–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
    "        return tokens\n",
    "\n",
    "\n",
    "    def save(self, filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            dill.dump(self, f)\n",
    "        print(f\"–û–±—ä–µ–∫—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {filename}\")\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            obj = dill.load(f)\n",
    "                \n",
    "        print(f\"–û–±—ä–µ–∫—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {filename}\")\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef121b7b",
   "metadata": {},
   "source": [
    "# –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ GPT-1: –ü—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞–±–æ—Ç—ã –∏ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã\n",
    "\n",
    "![](https://ucarecdn.com/b2551e49-de5c-490b-a371-9c4a81e35329/)\n",
    "\n",
    "–ú–æ–¥–µ–ª—å **GPT-1 (Generative Pretrained Transformer)** ‚Äî —ç—Ç–æ –ø–µ—Ä–≤–∞—è –≤–µ—Ä—Å–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Å–µ–º–µ–π—Å—Ç–≤–∞ GPT, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ **–¥–µ–∫–æ–¥–µ—Ä–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞**.  \n",
    "–û–Ω–∞ –±—ã–ª–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è–º–∏ **OpenAI** –≤ 2018 –≥–æ–¥—É –∏ —Å—Ç–∞–ª–∞ –æ—Å–Ω–æ–≤–æ–π –¥–ª—è –≤—Å–µ—Ö –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π, –≤–∫–ª—é—á–∞—è GPT-2, GPT-3 –∏ GPT-4.  \n",
    "\n",
    "–ì–ª–∞–≤–Ω–∞—è –∏–¥–µ—è GPT-1 –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ –º–æ–¥–µ–ª—å –º–æ–∂–Ω–æ –æ–±—É—á–∏—Ç—å **–ø–æ–Ω–∏–º–∞—Ç—å –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç**, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–∞—É—á–∏—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å **—Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω** –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.  \n",
    "–≠—Ç–æ—Ç –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–∏–Ω—Ü–∏–ø –ø–æ–∑–≤–æ–ª–∏–ª —Å–æ–∑–¥–∞—Ç—å —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—É—é —è–∑—ã–∫–æ–≤—É—é –º–æ–¥–µ–ª—å, —Å–ø–æ—Å–æ–±–Ω—É—é —Ä–µ—à–∞—Ç—å –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∑–∞–¥–∞—á –±–µ–∑ —Ä—É—á–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–¥ –∫–∞–∂–¥—É—é –∏–∑ –Ω–∏—Ö.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d11c5c",
   "metadata": {},
   "source": [
    "## 1. –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ (Embeddings)\n",
    "\n",
    "\n",
    "![](https://ucarecdn.com/4ce51ba3-83fc-46c3-a6e8-efa064663df0/)\n",
    "\n",
    "–ü–µ—Ä–µ–¥ —Ç–µ–º –∫–∞–∫ —Ç–µ–∫—Å—Ç –ø–æ–¥–∞–µ—Ç—Å—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä, –µ–≥–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ —á–∏—Å–ª–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ.  \n",
    "–≠—Ç–æ –¥–µ–ª–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é **—ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤** ‚Äî –ø–ª–æ—Ç–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∫–æ–¥–∏—Ä—É—é—Ç —Å–º—ã—Å–ª –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å–ª–æ–≤.\n",
    "\n",
    "GPT –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤–∞ —Ç–∏–ø–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e046ecc8",
   "metadata": {},
   "source": [
    "### 1.1 Token Embeddings\n",
    "\n",
    "–ö–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –∏–ª–∏ –ø–æ–¥—Å–ª–æ–≤–æ —Å–Ω–∞—á–∞–ª–∞ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç—Å—è –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç—Å—è –≤ —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —á–∏—Å–ª–æ–≤–æ–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä.  \n",
    "–ó–∞—Ç–µ–º —ç—Ç–æ—Ç –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è —Å –≤–µ–∫—Ç–æ—Ä–æ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã ‚Äî **—ç–º–±–µ–¥–¥–∏–Ω–≥–æ–º —Ç–æ–∫–µ–Ω–∞**.  \n",
    "\n",
    "–í–µ–∫—Ç–æ—Ä –º–æ–∂–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –∫–∞–∫ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Å–ª–æ–≤–∞ –≤ –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Å–º—ã—Å–ª–æ–≤: —Ç–æ–∫–µ–Ω—ã, –±–ª–∏–∑–∫–∏–µ –ø–æ –∑–Ω–∞—á–µ–Ω–∏—é, —Ä–∞—Å–ø–æ–ª–∞–≥–∞—é—Ç—Å—è —Ä—è–¥–æ–º.\n",
    "\n",
    "–§–æ—Ä–º–∞–ª—å–Ω–æ:\n",
    "$$\n",
    "E_{token}(t_i) = W_e[t_i]\n",
    "$$\n",
    "\n",
    "–≥–¥–µ  \n",
    "$W_e$ ‚Äî –æ–±—É—á–∞–µ–º–∞—è –º–∞—Ç—Ä–∏—Ü–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (—Ä–∞–∑–º–µ—Ä–æ–º `vocab_size √ó d_model`),  \n",
    "$t_i$ ‚Äî –∏–Ω–¥–µ–∫—Å —Ç–æ–∫–µ–Ω–∞ –≤ —Å–ª–æ–≤–∞—Ä–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1464a012",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size: int):\n",
    "        super().__init__()\n",
    "        self._embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=emb_size,\n",
    "            padding_idx=0  # —á—Ç–æ–±—ã 0 –º–æ–∂–Ω–æ –±—ã–ª–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self._embedding(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e6025b",
   "metadata": {},
   "source": [
    "### 1.2 Positional Embeddings\n",
    "\n",
    "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –Ω–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç –ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤, —Ç–∞–∫ –∫–∞–∫ –≤–Ω–∏–º–∞–Ω–∏–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ —Ç–æ–∫–µ–Ω—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ.  \n",
    "–ß—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –≤–≤–æ–¥—è—Ç—Å—è **–ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏**, –∫–æ—Ç–æ—Ä—ã–µ –¥–æ–±–∞–≤–ª—è—é—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–∑–∏—Ü–∏–∏ –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞.\n",
    "\n",
    "–í GPT-1 –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è **—Å–∏–Ω—É—Å–æ–∏–¥—ã —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ñ–æ—Ä–º—ã**:\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right), \\quad\n",
    "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
    "$$\n",
    "\n",
    "–û–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞:\n",
    "$$\n",
    "x_i = E_{token}(t_i) + PE(pos_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94ddd50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddings(nn.Module):\n",
    "    def __init__(self, max_seq_len: int, emb_size: int):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.emb_size = emb_size\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=max_seq_len,\n",
    "            embedding_dim=emb_size\n",
    "        )\n",
    "\n",
    "    def forward(self, seq_len: int) -> torch.Tensor:\n",
    "        if seq_len < 1 or seq_len > self.max_seq_len:\n",
    "            raise IndexError(f\"–î–ª–∏–Ω–∞ {seq_len} –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –æ—Ç 1 –¥–æ {self.max_seq_len}\")\n",
    "        positions = torch.arange(seq_len, device=self.embedding.weight.device)\n",
    "        return self.embedding(positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b04dff2",
   "metadata": {},
   "source": [
    "## 2. –í–Ω–∏–º–∞–Ω–∏–µ (Attention)\n",
    "\n",
    "\n",
    "![](https://ucarecdn.com/538faeda-c12d-4137-9f67-e87720d83e13/)\n",
    "\n",
    "–ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è ‚Äî –∫–ª—é—á–µ–≤–∞—è –∏–¥–µ—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤.  \n",
    "–û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ **–≤–∑–≤–µ—à–∏–≤–∞—Ç—å –≤–∞–∂–Ω–æ—Å—Ç—å –¥—Ä—É–≥–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤** –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—É—â–µ–≥–æ, —Ç–æ –µ—Å—Ç—å —Ä–µ—à–∞—Ç—å, –Ω–∞ –∫–∞–∫–∏–µ —Å–ª–æ–≤–∞ –Ω—É–∂–Ω–æ –æ–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å–ª–µ–¥—É—é—â–µ–≥–æ.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1 –ú–∞—Ç—Ä–∏—Ü–∞ –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "\n",
    "–ö–∞–∂–¥–æ–º—É —Ç–æ–∫–µ–Ω—É —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è—é—Ç—Å—è —Ç—Ä–∏ –æ–±—É—á–∞–µ–º—ã—Ö –≤–µ–∫—Ç–æ—Ä–∞:\n",
    "- **Query (Q)** ‚Äî –∑–∞–ø—Ä–æ—Å: —á—Ç–æ —Ç–µ–∫—É—â–∏–π —Ç–æ–∫–µ–Ω –∏—â–µ—Ç –≤ –¥—Ä—É–≥–∏—Ö;\n",
    "- **Key (K)** ‚Äî –∫–ª—é—á: –∫–∞–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –Ω–µ—Å–µ—Ç —Ç–æ–∫–µ–Ω;\n",
    "- **Value (V)** ‚Äî –∑–Ω–∞—á–µ–Ω–∏–µ: —Å–∞–º–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ç–æ–∫–µ–Ω–∞.\n",
    "\n",
    "–≠—Ç–∏ –≤–µ–∫—Ç–æ—Ä—ã –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –ª–∏–Ω–µ–π–Ω—ã–º–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è–º–∏ –≤—Ö–æ–¥–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤:\n",
    "$$\n",
    "Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V\n",
    "$$\n",
    "\n",
    "–ó–∞—Ç–µ–º –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è **–≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ**:\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 –ú–∞—Ç—Ä–∏—á–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏\n",
    "\n",
    "- $QK^T$ ‚Äî –º–∞—Ç—Ä–∏—Ü–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–µ–∂–¥—É —Ç–æ–∫–µ–Ω–∞–º–∏;  \n",
    "- –¥–µ–ª–µ–Ω–∏–µ –Ω–∞ $\\sqrt{d_k}$ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã;  \n",
    "- **softmax** –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç –æ—Ü–µ–Ω–∫–∏ –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏;  \n",
    "- —É–º–Ω–æ–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –Ω–∞ $V$ –¥–∞—ë—Ç –≤–∑–≤–µ—à–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fe8d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size: int, head_size: int, max_seq_len: int):\n",
    "        super().__init__()\n",
    "        self._emb_size = emb_size\n",
    "        self._head_size = head_size\n",
    "        self._max_seq_len = max_seq_len\n",
    "\n",
    "        # –õ–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–ª—è Q, K, V\n",
    "        self._k = nn.Linear(emb_size, head_size)\n",
    "        self._q = nn.Linear(emb_size, head_size)\n",
    "        self._v = nn.Linear(emb_size, head_size)\n",
    "\n",
    "        # –°–æ–∑–¥–∞–Ω–∏–µ causal –º–∞—Å–∫–∏\n",
    "        mask = torch.tril(torch.ones(max_seq_len, max_seq_len))\n",
    "        self.register_buffer('_tril_mask', mask.bool() if hasattr(torch, 'bool') else mask.byte())\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        seq_len = x.shape[1]\n",
    "        if seq_len > self._max_seq_len:\n",
    "            raise ValueError(f\"–î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ {seq_len} –ø—Ä–µ–≤—ã—à–∞–µ—Ç –º–∞–∫—Å–∏–º—É–º {self._max_seq_len}\")\n",
    "\n",
    "        # 1. –õ–∏–Ω–µ–π–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è\n",
    "        k = self._k(x)  # [B, T, hs]\n",
    "        q = self._q(x)  # [B, T, hs]\n",
    "        \n",
    "        # 2. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ scores\n",
    "        scores = q @ k.transpose(-2, -1) / self._head_size ** 0.5\n",
    "        \n",
    "        # 3. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ causal –º–∞—Å–∫–∏\n",
    "        scores = scores.masked_fill(~self._tril_mask[:seq_len, :seq_len], float('-inf'))\n",
    "        \n",
    "        # 4. Softmax –∏ —É–º–Ω–æ–∂–µ–Ω–∏–µ –Ω–∞ V\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        return weights @ self._v(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a920fbcf",
   "metadata": {},
   "source": [
    "### 2.3 –ú—É–ª—å—Ç–∏—Ö–µ–¥ (Multi-Head Attention)\n",
    "\n",
    "–í–º–µ—Å—Ç–æ –æ–¥–Ω–æ–π –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤–Ω–∏–º–∞–Ω–∏—è GPT-1 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö **–≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è**.  \n",
    "–ö–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–∞—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ ‚Äî —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–µ, —Å–µ–º–∞–Ω—Ç–∏–∫–µ –∏–ª–∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è—Ö.\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h)W_O\n",
    "$$\n",
    "\n",
    "–≥–¥–µ –∫–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞:\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW_{Qi}, KW_{Ki}, VW_{Vi})\n",
    "$$\n",
    "\n",
    "–í GPT-1 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è **12 –≥–æ–ª–æ–≤**, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–≥–∞—Ç–æ–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ —Ç–µ–∫—Å—Ç–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d55276a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, emb_size: int, head_size: int, max_seq_len: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self._heads = nn.ModuleList([\n",
    "            HeadAttention(\n",
    "                emb_size=emb_size, \n",
    "                head_size=head_size, \n",
    "                max_seq_len=max_seq_len\n",
    "            ) for _ in range(num_heads)\n",
    "        ])\n",
    "        self._layer = nn.Linear(head_size * num_heads, emb_size)\n",
    "        self._dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None):\n",
    "        # 1. –í—ã—á–∏—Å–ª—è–µ–º attention –¥–ª—è –∫–∞–∂–¥–æ–π –≥–æ–ª–æ–≤—ã\n",
    "        attention_outputs = [head(x) for head in self._heads]\n",
    "        \n",
    "        # 2. –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö –≥–æ–ª–æ–≤\n",
    "        concatenated_attention = torch.cat(attention_outputs, dim=-1)\n",
    "        \n",
    "        # 3. –ü—Ä–æ–µ—Ü–∏—Ä—É–µ–º –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "        projected_output = self._layer(concatenated_attention)\n",
    "        \n",
    "        # 4. –ü—Ä–∏–º–µ–Ω—è–µ–º dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏\n",
    "        final_output = self._dropout(projected_output)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffafb56",
   "metadata": {},
   "source": [
    "## 3. Feed Forward Network (FFN)\n",
    "\n",
    "![](https://ucarecdn.com/6af52549-95fa-45be-9764-9c399f387aa6/)\n",
    "\n",
    "–ü–æ—Å–ª–µ –±–ª–æ–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –ø—Ä–æ—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ –¥–≤—É—Ö—Å–ª–æ–π–Ω—É—é –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å ‚Äî **Feed Forward Network**.  \n",
    "–û–Ω–∞ –¥–æ–±–∞–≤–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –Ω–µ–ª–∏–Ω–µ–π–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤—ã–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è **ReLU**.  \n",
    "FFN –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –æ–¥–∏–Ω–∞–∫–æ–≤–æ –∫ –∫–∞–∂–¥–æ–º—É —Ç–æ–∫–µ–Ω—É –∏ –Ω–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –ø–æ—Ä—è–¥–∫–∞ —Å–ª–æ–≤, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º–∏.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84f57562",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, emb_size: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(emb_size, 4 * emb_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * emb_size, emb_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        self.net = self.net.to(x.dtype)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9ce9d8",
   "metadata": {},
   "source": [
    "## 4. –ë–ª–æ–∫ –î–µ–∫–æ–¥–µ—Ä–∞\n",
    "\n",
    "\n",
    "![](https://ucarecdn.com/c7cc9bf3-cc75-4fac-97a9-ce122c74738e/)\n",
    "\n",
    "\n",
    "–ö–∞–∂–¥—ã–π —Å–ª–æ–π GPT-1 ‚Äî —ç—Ç–æ **–¥–µ–∫–æ–¥–µ—Ä**, —Å–æ—Å—Ç–æ—è—â–∏–π –∏–∑ —Å–ª–µ–¥—É—é—â–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤:\n",
    "\n",
    "1. **Masked Multi-Head Attention**  \n",
    "   –ú–∞—Å–∫–∞ –∑–∞–ø—Ä–µ—â–∞–µ—Ç —Ç–æ–∫–µ–Ω—É –≤–∏–¥–µ—Ç—å –±—É–¥—É—â–∏–µ –ø–æ–∑–∏—Ü–∏–∏, —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∞–≤—Ç–æ—Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (—Å–ª–µ–≤–∞ –Ω–∞–ø—Ä–∞–≤–æ).\n",
    "\n",
    "2. **–û—Å—Ç–∞—Ç–æ—á–Ω—ã–µ —Å–≤—è–∑–∏ + Layer Normalization**  \n",
    "   –†–µ–∑—É–ª—å—Ç–∞—Ç –≤–Ω–∏–º–∞–Ω–∏—è —Å–∫–ª–∞–¥—ã–≤–∞–µ—Ç—Å—è —Å –≤—Ö–æ–¥–æ–º —Å–ª–æ—è, –∞ –∑–∞—Ç–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç—Å—è:  \n",
    "   $$\n",
    "   x' = \\text{LayerNorm}(x + \\text{Attention}(x))\n",
    "   $$\n",
    "\n",
    "3. **Feed Forward + Residual + LayerNorm**  \n",
    "   $$\n",
    "   y = \\text{LayerNorm}(x' + \\text{FFN}(x'))\n",
    "   $$\n",
    "\n",
    "GPT-1 —Å–æ–¥–µ—Ä–∂–∏—Ç **12 —Ç–∞–∫–∏—Ö –±–ª–æ–∫–æ–≤**, —Å–æ–µ–¥–∏–Ω—ë–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "300acc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "        num_heads: int,\n",
    "        emb_size: int,\n",
    "        head_size: int,\n",
    "        max_seq_len: int,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._heads = MultiHeadAttention(\n",
    "            num_heads=num_heads, \n",
    "            emb_size=emb_size, \n",
    "            head_size=head_size, \n",
    "            max_seq_len=max_seq_len, \n",
    "            dropout=dropout\n",
    "        )\n",
    "        self._ff = FeedForward(\n",
    "            emb_size=emb_size, \n",
    "            dropout=dropout\n",
    "        )\n",
    "        self._norm1 = nn.LayerNorm(emb_size)\n",
    "        self._norm2 = nn.LayerNorm(emb_size)\n",
    "        #self._dropout_attn = nn.Dropout(dropout)\n",
    "        #self._dropout_ffn = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None):\n",
    "        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "        self._heads = self._heads.to(x.dtype)\n",
    "        self._ff = self._ff.to(x.dtype)\n",
    "        \n",
    "        # –ü—Ä–æ–ø—É—Å—Ç–∏–º —Ç–µ–Ω–∑–æ—Ä x —á–µ—Ä–µ–∑ —ç–∫–∑–µ–º–ø–ª—è—Ä MultiHeadAttention.\n",
    "        attention = self._heads(x, mask)\n",
    "        #attention = self._dropout_attn(attention)\n",
    "        \n",
    "        # –í—ã—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä –∏–∑ –±–ª–æ–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è —Å–ª–æ–∂–∏–º —Å –∏—Å—Ö–æ–¥–Ω—ã–º x.\n",
    "        out = attention + x\n",
    "        \n",
    "        # –ü–æ–ª—É—á–∏–≤—à–∏–π—Å—è —Ç–µ–Ω–∑–æ—Ä –ø—Ä–æ–ø—É—Å—Ç–∏–º —á–µ—Ä–µ–∑ –ø–µ—Ä–≤—ã–π —Å–ª–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.\n",
    "        norm_out = self._norm1(out)\n",
    "        \n",
    "        # –ó–∞—Ç–µ–º –ø–æ–¥–∞–¥–∏–º –µ–≥–æ –Ω–∞ –≤—Ö–æ–¥ —ç–∫–∑–µ–º–ø–ª—è—Ä—É FFN.\n",
    "        ffn_out = self._ff(norm_out)\n",
    "        #ffn_out = self._dropout_ffn(ffn_out)\n",
    "        \n",
    "        # –í—ã—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä –∏–∑ FFN —Å–ª–æ–∂–∏–º —Å —Ç–µ–º, —á—Ç–æ –ø–æ—Å—Ç—É–ø–∏–ª –Ω–∞ –≤—Ö–æ–¥.\n",
    "        out = ffn_out + norm_out\n",
    "        \n",
    "        # –ü—Ä–æ–ø—É—Å—Ç–∏–º –ø–æ–ª—É—á–∏–≤—à–∏–π—Å—è —Ç–µ–Ω–∑–æ—Ä —á–µ—Ä–µ–∑ –≤—Ç–æ—Ä–æ–π —Å–ª–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.\n",
    "        norm_out = self._norm2(out)\n",
    "        \n",
    "        # –í–µ—Ä–Ω–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Ç–µ–Ω–∑–æ—Ä —Ä–∞–∑–º–µ—Ä–æ–º batch_size x seq_len x emb_size.\n",
    "        return norm_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888d1a1c",
   "metadata": {},
   "source": [
    "## 5. –û–±—É—á–µ–Ω–∏–µ GPT-1\n",
    "\n",
    "GPT-1 –æ–±—É—á–∞–µ—Ç—Å—è –≤ –¥–≤–∞ —ç—Ç–∞–ø–∞:\n",
    "\n",
    "- 1Ô∏è‚É£ **–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ (Unsupervised Pretraining)**  \n",
    "- 2Ô∏è‚É£ **–î–æ–æ–±—É—á–µ–Ω–∏–µ (Supervised Fine-Tuning)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eb26ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"GPT-like —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞\n",
    "    \n",
    "    Args:\n",
    "        vocab_size: –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è\n",
    "        max_seq_len: –ú–∞–∫—Å. –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "        emb_size: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "        num_heads: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "        head_size: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è\n",
    "        num_layers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤ –¥–µ–∫–æ–¥–µ—Ä–∞\n",
    "        dropout: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å dropout (default=0.1)\n",
    "        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (default='cpu')\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        vocab_size: int,\n",
    "        max_seq_len: int,\n",
    "        emb_size: int,\n",
    "        num_heads: int,\n",
    "        head_size: int,\n",
    "        num_layers: int,\n",
    "        dropout: float = 0.1,\n",
    "        device: str = 'cpu'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ–µ–≤\n",
    "        self._token_embeddings = TokenEmbeddings(\n",
    "            vocab_size=vocab_size, \n",
    "            emb_size=emb_size\n",
    "        )\n",
    "        self._position_embeddings = PositionEmbeddings(\n",
    "            max_seq_len=max_seq_len, \n",
    "            emb_size=emb_size\n",
    "        )\n",
    "        self._dropout = nn.Dropout(dropout)\n",
    "        self._decoders = nn.ModuleList([Decoder(\n",
    "            num_heads=num_heads,\n",
    "            emb_size=emb_size,\n",
    "            head_size=head_size,\n",
    "            max_seq_len=max_seq_len,\n",
    "            dropout=dropout \n",
    "        ) for _ in range(num_layers)])\n",
    "        self._linear = nn.Linear(emb_size, vocab_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ GPT\n",
    "        \n",
    "        Args:\n",
    "            x: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä [batch_size, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            –¢–µ–Ω–∑–æ—Ä –ª–æ–≥–∏—Ç–æ–≤ [batch_size, seq_len, vocab_size]\n",
    "        \"\"\"\n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "        if x.size(1) > self.max_seq_len:\n",
    "            raise ValueError(f\"–î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ {x.size(1)} –ø—Ä–µ–≤—ã—à–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é {self.max_seq_len}\")\n",
    "        \n",
    "        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–æ–∫–µ–Ω–æ–≤ –∏ –ø–æ–∑–∏—Ü–∏–π\n",
    "        tok_out = self._token_embeddings(x)  # [batch, seq_len, emb_size]\n",
    "        pos_out = self._position_embeddings(x.size(1))  # [seq_len, emb_size]\n",
    "        \n",
    "        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "        out = self._dropout(tok_out + pos_out.unsqueeze(0))  # [batch, seq_len, emb_size]\n",
    "        \n",
    "        # –°—Ç–µ–∫ –¥–µ–∫–æ–¥–µ—Ä–æ–≤\n",
    "        for decoder in self._decoders:\n",
    "            out = decoder(out)\n",
    "            \n",
    "        return self._linear(out)  # [batch, seq_len, vocab_size]\n",
    "\n",
    "    def generate(self, x: torch.Tensor, max_new_tokens: int):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # 1. –û–±—Ä–µ–∑–∞–µ–º –≤—Ö–æ–¥, –µ—Å–ª–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è\n",
    "            x_cond = x[:, -self.max_seq_len:]\n",
    "\n",
    "            # 2. –ü–µ—Ä–µ–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –º–µ—Ç–æ–¥ forward –∫–ª–∞—Å—Å–∞ GPT –∏ –ø–æ–ª—É–∞–µ–º –ª–æ–≥–∏—Ç—ã.\n",
    "            logits = self.forward(x_cond)\n",
    "\n",
    "            # 3. –ë–µ—Ä–µ–º –ª–æ–≥–∏—Ç—ã –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Ç–æ–∫–µ–Ω–∞\n",
    "            last_logits = logits[:, -1, :]  # [batch_size, vocab_size]\n",
    "\n",
    "            # 4. –ü—Ä–∏–º–µ–Ω—è–µ–º Softmax\n",
    "            probs = F.softmax(last_logits, dim=-1)  # [batch_size, vocab_size]\n",
    "\n",
    "            # 5. –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–∫–µ–Ω —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é\n",
    "            next_token = torch.argmax(probs, dim=-1, keepdim=True)  # [batch_size, 1]\n",
    "\n",
    "            # 6. –î–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ –∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "            x = torch.cat([x, next_token], dim=1)  # [batch_size, seq_len+1]     \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47966ba",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 5.1 –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ\n",
    "\n",
    "–ù–∞ –ø–µ—Ä–≤–æ–º —ç—Ç–∞–ø–µ –º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –±–µ–∑ —Ä–∞–∑–º–µ—Ç–∫–∏: –æ–Ω–∞ –ø–æ–ª—É—á–∞–µ—Ç –±–æ–ª—å—à–æ–π –∫–æ—Ä–ø—É—Å —Ç–µ–∫—Å—Ç–æ–≤ –∏ —É—á–∏—Ç—Å—è **–ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω** –ø–æ –ø—Ä–µ–¥—ã–¥—É—â–∏–º.\n",
    "\n",
    "–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å:\n",
    "$$\n",
    "L = - \\sum_{t=1}^{T} \\log P(x_t | x_1, x_2, ..., x_{t-1})\n",
    "$$\n",
    "\n",
    "–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è —Å—Ç—Ä–æ–∏—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—É—é –º–æ–¥–µ–ª—å —è–∑—ã–∫–∞, \"—É–≥–∞–¥—ã–≤–∞—è\" –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e4624e",
   "metadata": {},
   "source": [
    "–í–æ –≤—Ä–µ–º—è **–ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è** GPT-1 —É—á–∏—Ç—Å—è **–ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω** (language modeling task).  \n",
    "–§–æ—Ä–º–∞–ª—å–Ω–æ:  \n",
    "$$ \n",
    "P(x_t ,|, x_1, x_2, \\dots, x_{t-1})  \n",
    "$$ \n",
    "–¢–æ –µ—Å—Ç—å, –µ—Å–ª–∏ –Ω–∞ –≤—Ö–æ–¥ –ø–æ–¥–∞—ë—Ç—Å—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ `\"I love deep\"`, –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å `\"learning\"`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dcc10e",
   "metadata": {},
   "source": [
    "### ‚úÖ 5.1.1 –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "–°–æ–∑–¥–∞–¥–∏–º **–¥–∞—Ç–∞—Å–µ—Ç** –Ω–∞ –æ—Å–Ω–æ–≤–µ BPE-—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "632eec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, text: str, bpe: BPE, block_size: int):\n",
    "        self.bpe = bpe\n",
    "        self.block_size = block_size\n",
    "        self.data = bpe.encode(text)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx:idx+self.block_size], dtype=torch.long)\n",
    "        y = torch.tensor(self.data[idx+1:idx+self.block_size+1], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5d83d8",
   "metadata": {},
   "source": [
    "- `x` ‚Äî –≤—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤\n",
    "    \n",
    "- `y` ‚Äî —Ç–∞ –∂–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –Ω–æ —Å–¥–≤–∏–Ω—É—Ç–∞—è –Ω–∞ –æ–¥–∏–Ω —Ç–æ–∫–µ–Ω –≤–ø–µ—Ä—ë–¥ (—Ü–µ–ª—å)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24de37be",
   "metadata": {},
   "source": [
    "### ‚úÖ 5.1.2 –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "–î–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–æ–∑–¥–∞–¥–∏–º —Ñ—É–Ω–∫—Ü–∏—é:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8003ea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "def train_gpt(model, dataset, epochs=5, batch_size=32, lr=3e-4, device='cpu'):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥\n",
    "            logits = model(x)  # [B, T, vocab_size]\n",
    "\n",
    "            # –ü–µ—Ä–µ—Å—Ç—Ä–æ–∏–º –≤—ã—Ö–æ–¥ –ø–æ–¥ CrossEntropy\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "            # –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c351b56",
   "metadata": {},
   "source": [
    "### ‚úÖ 5.1.3 –ü—Ä–∏–º–µ—Ä –∑–∞–ø—É—Å–∫–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd700a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 20\n",
      "Epoch 1/100, Loss: 4.5466\n",
      "Epoch 2/100, Loss: 4.2532\n",
      "Epoch 3/100, Loss: 3.9998\n",
      "Epoch 4/100, Loss: 3.7857\n",
      "Epoch 5/100, Loss: 3.5823\n",
      "Epoch 6/100, Loss: 3.3802\n",
      "Epoch 7/100, Loss: 3.2312\n",
      "Epoch 8/100, Loss: 3.0589\n",
      "Epoch 9/100, Loss: 2.8971\n",
      "Epoch 10/100, Loss: 2.7329\n",
      "Epoch 11/100, Loss: 2.5889\n",
      "Epoch 12/100, Loss: 2.4817\n",
      "Epoch 13/100, Loss: 2.3101\n",
      "Epoch 14/100, Loss: 2.1343\n",
      "Epoch 15/100, Loss: 2.0490\n",
      "Epoch 16/100, Loss: 1.8943\n",
      "Epoch 17/100, Loss: 1.7862\n",
      "Epoch 18/100, Loss: 1.6848\n",
      "Epoch 19/100, Loss: 1.5660\n",
      "Epoch 20/100, Loss: 1.4896\n",
      "Epoch 21/100, Loss: 1.3954\n",
      "Epoch 22/100, Loss: 1.3091\n",
      "Epoch 23/100, Loss: 1.2422\n",
      "Epoch 24/100, Loss: 1.1602\n",
      "Epoch 25/100, Loss: 1.1006\n",
      "Epoch 26/100, Loss: 1.0547\n",
      "Epoch 27/100, Loss: 0.9972\n",
      "Epoch 28/100, Loss: 0.9414\n",
      "Epoch 29/100, Loss: 0.8983\n",
      "Epoch 30/100, Loss: 0.8630\n",
      "Epoch 31/100, Loss: 0.7975\n",
      "Epoch 32/100, Loss: 0.7723\n",
      "Epoch 33/100, Loss: 0.7307\n",
      "Epoch 34/100, Loss: 0.7038\n",
      "Epoch 35/100, Loss: 0.6767\n",
      "Epoch 36/100, Loss: 0.6498\n",
      "Epoch 37/100, Loss: 0.6052\n",
      "Epoch 38/100, Loss: 0.5923\n",
      "Epoch 39/100, Loss: 0.5587\n",
      "Epoch 40/100, Loss: 0.5362\n",
      "Epoch 41/100, Loss: 0.5186\n",
      "Epoch 42/100, Loss: 0.4819\n",
      "Epoch 43/100, Loss: 0.4704\n",
      "Epoch 44/100, Loss: 0.4753\n",
      "Epoch 45/100, Loss: 0.4510\n",
      "Epoch 46/100, Loss: 0.4102\n",
      "Epoch 47/100, Loss: 0.3981\n",
      "Epoch 48/100, Loss: 0.3920\n",
      "Epoch 49/100, Loss: 0.3864\n",
      "Epoch 50/100, Loss: 0.3532\n",
      "Epoch 51/100, Loss: 0.3462\n",
      "Epoch 52/100, Loss: 0.3315\n",
      "Epoch 53/100, Loss: 0.3281\n",
      "Epoch 54/100, Loss: 0.3150\n",
      "Epoch 55/100, Loss: 0.3121\n",
      "Epoch 56/100, Loss: 0.3134\n",
      "Epoch 57/100, Loss: 0.2914\n",
      "Epoch 58/100, Loss: 0.2914\n",
      "Epoch 59/100, Loss: 0.2678\n",
      "Epoch 60/100, Loss: 0.2641\n",
      "Epoch 61/100, Loss: 0.2631\n",
      "Epoch 62/100, Loss: 0.2479\n",
      "Epoch 63/100, Loss: 0.2349\n",
      "Epoch 64/100, Loss: 0.2383\n",
      "Epoch 65/100, Loss: 0.2283\n",
      "Epoch 66/100, Loss: 0.2229\n",
      "Epoch 67/100, Loss: 0.2152\n",
      "Epoch 68/100, Loss: 0.2116\n",
      "Epoch 69/100, Loss: 0.2042\n",
      "Epoch 70/100, Loss: 0.1961\n",
      "Epoch 71/100, Loss: 0.1787\n",
      "Epoch 72/100, Loss: 0.1907\n",
      "Epoch 73/100, Loss: 0.1777\n",
      "Epoch 74/100, Loss: 0.1813\n",
      "Epoch 75/100, Loss: 0.1711\n",
      "Epoch 76/100, Loss: 0.1836\n",
      "Epoch 77/100, Loss: 0.1748\n",
      "Epoch 78/100, Loss: 0.1684\n",
      "Epoch 79/100, Loss: 0.1622\n",
      "Epoch 80/100, Loss: 0.1739\n",
      "Epoch 81/100, Loss: 0.1607\n",
      "Epoch 82/100, Loss: 0.1657\n",
      "Epoch 83/100, Loss: 0.1579\n",
      "Epoch 84/100, Loss: 0.1588\n",
      "Epoch 85/100, Loss: 0.1526\n",
      "Epoch 86/100, Loss: 0.1405\n",
      "Epoch 87/100, Loss: 0.1420\n",
      "Epoch 88/100, Loss: 0.1531\n",
      "Epoch 89/100, Loss: 0.1392\n",
      "Epoch 90/100, Loss: 0.1355\n",
      "Epoch 91/100, Loss: 0.1278\n",
      "Epoch 92/100, Loss: 0.1331\n",
      "Epoch 93/100, Loss: 0.1343\n",
      "Epoch 94/100, Loss: 0.1355\n",
      "Epoch 95/100, Loss: 0.1298\n",
      "Epoch 96/100, Loss: 0.1254\n",
      "Epoch 97/100, Loss: 0.1149\n",
      "Epoch 98/100, Loss: 0.1265\n",
      "Epoch 99/100, Loss: 0.1308\n",
      "Epoch 100/100, Loss: 0.1178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (_token_embeddings): TokenEmbeddings(\n",
       "    (_embedding): Embedding(100, 64, padding_idx=0)\n",
       "  )\n",
       "  (_position_embeddings): PositionEmbeddings(\n",
       "    (embedding): Embedding(8, 64)\n",
       "  )\n",
       "  (_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (_decoders): ModuleList(\n",
       "    (0-1): 2 x Decoder(\n",
       "      (_heads): MultiHeadAttention(\n",
       "        (_heads): ModuleList(\n",
       "          (0-3): 4 x HeadAttention(\n",
       "            (_k): Linear(in_features=64, out_features=16, bias=True)\n",
       "            (_q): Linear(in_features=64, out_features=16, bias=True)\n",
       "            (_v): Linear(in_features=64, out_features=16, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (_layer): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (_ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (_norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (_norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (_linear): Linear(in_features=64, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
    "text = \"Deep learning is amazing. Transformers changed the world. Attention is all you need. GPT models revolutionized NLP.\"\n",
    "\n",
    "# 2. –û–±—É—á–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "bpe = BPE(vocab_size=100)\n",
    "bpe.fit(text)\n",
    "\n",
    "# 3. –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç\n",
    "dataset = GPTDataset(text, bpe, block_size=8)\n",
    "print(f\"Dataset length: {len(dataset)}\")\n",
    "\n",
    "# 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å\n",
    "gpt = GPT(\n",
    "    vocab_size=len(bpe.vocab),\n",
    "    max_seq_len=8,\n",
    "    emb_size=64,\n",
    "    num_heads=4,\n",
    "    head_size=16,\n",
    "    num_layers=2,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# 5. –û–±—É—á–∞–µ–º\n",
    "train_gpt(gpt, dataset, epochs=100, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3714dfc",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### 5.2 –î–æ–æ–±—É—á–µ–Ω–∏–µ\n",
    "\n",
    "–ü–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è GPT-1 —É–∂–µ –∑–Ω–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ –≥—Ä–∞–º–º–∞—Ç–∏–∫—É —è–∑—ã–∫–∞.  \n",
    "–ù–∞ –≤—Ç–æ—Ä–æ–º —ç—Ç–∞–ø–µ –æ–Ω–∞ –¥–æ–æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, QA) —Å –ø–æ–º–æ—â—å—é —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.\n",
    "\n",
    "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏ —ç—Ç–æ –ø–æ—á—Ç–∏ —Ç–æ –∂–µ –æ–±—É—á–µ–Ω–∏–µ, —Ç–æ–ª—å–∫–æ:\n",
    "\n",
    "- –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å —É–∂–µ –æ–±—É—á–µ–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏.\n",
    "- –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
    "- –ú–æ–∂–Ω–æ —É–º–µ–Ω—å—à–∏—Ç—å —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è.\n",
    "- –ò–Ω–æ–≥–¥–∞ –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞—é—Ç —á–∞—Å—Ç—å —Å–ª–æ—ë–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —ç–º–±–µ–¥–¥–∏–Ω–≥–∏).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4afd7733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_gpt(model, dataset, epochs=3, batch_size=16, lr=1e-5, device='cpu', freeze_embeddings=True):\n",
    "    if freeze_embeddings:\n",
    "        for param in model._token_embeddings.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model._position_embeddings.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Fine-tune Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1698def",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71bb6b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune Epoch 1/10, Loss: 4.3808\n",
      "Fine-tune Epoch 2/10, Loss: 3.9245\n",
      "Fine-tune Epoch 3/10, Loss: 3.5217\n",
      "Fine-tune Epoch 4/10, Loss: 3.2451\n",
      "Fine-tune Epoch 5/10, Loss: 3.0076\n",
      "Fine-tune Epoch 6/10, Loss: 2.8133\n",
      "Fine-tune Epoch 7/10, Loss: 2.6857\n",
      "Fine-tune Epoch 8/10, Loss: 2.5984\n",
      "Fine-tune Epoch 9/10, Loss: 2.5168\n",
      "Fine-tune Epoch 10/10, Loss: 2.4128\n"
     ]
    }
   ],
   "source": [
    "# –ù–∞–ø—Ä–∏–º–µ—Ä, –º—ã —Ö–æ—Ç–∏–º –¥–æ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ —Å—Ç–∏–ª–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ñ—Ä–∞–∑\n",
    "fine_tune_text = \"\"\"\n",
    "Transformers revolutionize NLP.\n",
    "Deep learning enables self-attention.\n",
    "GPT generates text autoregressively.\n",
    "\"\"\"\n",
    "\n",
    "dataset = GPTDataset(fine_tune_text, bpe, block_size=8)\n",
    "\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫ –¥–æ–æ–±—É—á–µ–Ω–∏—è\n",
    "fine_tune_gpt(gpt, dataset, epochs=10, batch_size=4, lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ff63e9",
   "metadata": {},
   "source": [
    "## üìù 6. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccb9621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, bpe, prompt: str, max_new_tokens=20, device='cpu'):\n",
    "    model.eval()\n",
    "    ids = torch.tensor([bpe.encode(prompt)], dtype=torch.long).to(device)\n",
    "    out = model.generate(ids, max_new_tokens=max_new_tokens)\n",
    "    text = bpe.decode(out[0].tolist())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1b82472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep learning e els revolutionized NLP.\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(gpt, bpe, \"Deep learning\", max_new_tokens=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb376510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
